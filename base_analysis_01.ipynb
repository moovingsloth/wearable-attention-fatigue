{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import os\n",
    "\n",
    "\n",
    "DEFAULT_TZ = pytz.FixedOffset(540)  # GMT+09:00; Asia/Seoul\n",
    "\n",
    "PATH_DATA = './data'\n",
    "PATH_ESM = os.path.join(PATH_DATA, 'EsmResponse.csv')\n",
    "PATH_PARTICIPANT = os.path.join(PATH_DATA, 'UserInfo.csv')\n",
    "PATH_SENSOR = os.path.join(PATH_DATA, 'Sensor')\n",
    "\n",
    "PATH_INTERMEDIATE = './intermediate'\n",
    "\n",
    "DATA_TYPES = {\n",
    "    'EDA': 'EDA',\n",
    "    'HR': 'HRT',\n",
    "    'RRI': 'RRI',\n",
    "    'SkinTemperature': 'SKT',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import cloudpickle\n",
    "import ray\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "def load(path: str):\n",
    "    with open(path, mode='rb') as f:\n",
    "        return cloudpickle.load(f)\n",
    "\n",
    "    \n",
    "def dump(obj, path: str):\n",
    "    with open(path, mode='wb') as f:\n",
    "        cloudpickle.dump(obj, f)\n",
    "        \n",
    "    \n",
    "def log(msg: any):\n",
    "    print('[{}] {}'.format(datetime.now().strftime('%y-%m-%d %H:%M:%S'), msg))\n",
    "\n",
    "\n",
    "def summary(x):\n",
    "    x = np.asarray(x)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        n = len(x)\n",
    "        # Here, uppercase np.dtype.kind corresponds to non-numeric data.\n",
    "        # Also, we view the boolean data as dichotomous categorical data.\n",
    "        if x.dtype.kind.isupper() or x.dtype.kind == 'b': \n",
    "            cnt = pd.Series(x).value_counts(dropna=False)\n",
    "            card = len(cnt)\n",
    "            cnt = cnt[:20]                \n",
    "            cnt_str = ', '.join([f'{u}:{c}' for u, c in zip(cnt.index, cnt)])\n",
    "            if card > 30:\n",
    "                cnt_str = f'{cnt_str}, ...'\n",
    "            return {\n",
    "                'n': n,\n",
    "                'cardinality': card,\n",
    "                'value_count': cnt_str\n",
    "            }\n",
    "        else: \n",
    "            x_nan = x[np.isnan(x)]\n",
    "            x_norm = x[~np.isnan(x)]\n",
    "            \n",
    "            tot = np.sum(x_norm)\n",
    "            m = np.mean(x_norm)\n",
    "            me = np.median(x_norm)\n",
    "            s = np.std(x_norm, ddof=1)\n",
    "            l, u = np.min(x_norm), np.max(x)\n",
    "            conf_l, conf_u = st.t.interval(0.95, len(x_norm) - 1, loc=m, scale=st.sem(x_norm))\n",
    "            n_nan = len(x_nan)\n",
    "            \n",
    "            return {\n",
    "                'n': n,\n",
    "                'sum': tot,\n",
    "                'mean': m,\n",
    "                'SD': s,\n",
    "                'med': me,\n",
    "                'range': (l, u),\n",
    "                'conf.': (conf_l, conf_u),\n",
    "                'nan_count': n_nan\n",
    "            }\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def on_ray(*args, **kwargs):\n",
    "    try:\n",
    "        if ray.is_initialized():\n",
    "            ray.shutdown()\n",
    "        ray.init(*args, **kwargs)\n",
    "        yield None\n",
    "    finally:\n",
    "        ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n",
      "✔ dplyr     1.1.4     ✔ readr     2.1.5\n",
      "✔ forcats   1.0.0     ✔ stringr   1.5.1\n",
      "✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n",
      "✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n",
      "✔ purrr     1.0.4     \n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: 필요한 패키지를 로딩중입니다: sysfonts\n",
      "\n",
      "R[write to console]: 필요한 패키지를 로딩중입니다: showtextdb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(tidyverse)\n",
    "library(ggforce)\n",
    "library(ggpubr)\n",
    "library(showtext)\n",
    "library(rmcorr)\n",
    "library(patchwork)\n",
    "\n",
    "# font_add_google(\n",
    "#     name='Source Serif Pro',\n",
    "#     family='ssp',\n",
    "#     db_cache=FALSE\n",
    "# )\n",
    "\n",
    "showtext_auto()\n",
    "\n",
    "THEME_DEFAULT <- theme_bw(\n",
    "    base_size=10,\n",
    "    base_family='ssp',\n",
    ") + theme(\n",
    "        axis.title.x=element_text(colour='grey20', size=10, face='bold'),\n",
    "        axis.title.y=element_text(colour='grey20', size=10, face='bold'),\n",
    "        axis.text.x=element_text(colour='grey20', size=10),\n",
    "        axis.text.y=element_text(colour='grey20', size=10),\n",
    "        strip.text.x=element_text(colour='grey20', size=10, face='bold'),\n",
    "        strip.text.y=element_text(colour='grey20', size=10, face='bold'),\n",
    "        legend.background=element_blank(),\n",
    "        legend.title=element_text(colour='grey20', size=10, face='bold'),\n",
    "        legend.text=element_text(colour='grey20', size=10),\n",
    "        legend.position='top',\n",
    "        legend.box.spacing= unit(0, 'cm'),\n",
    "        plot.subtitle=element_text(colour='grey20', size=10, hjust=.5),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participationStartDate</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>PSS</th>\n",
       "      <th>PHQ</th>\n",
       "      <th>GHQ</th>\n",
       "      <th>particpationStartDateTime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P02</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>21</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P03</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P04</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P05</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>27</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      participationStartDate  age gender  openness  conscientiousness  \\\n",
       "pcode                                                                   \n",
       "P01               2019-05-08   27      M        11                 11   \n",
       "P02               2019-05-08   21      M        14                  5   \n",
       "P03               2019-05-08   24      F        10                 15   \n",
       "P04               2019-05-08   23      M        12                 11   \n",
       "P05               2019-05-08   27      F        10                 11   \n",
       "\n",
       "       neuroticism  extraversion  agreeableness  PSS  PHQ  GHQ  \\\n",
       "pcode                                                            \n",
       "P01              3             4             13   13    0    1   \n",
       "P02             12            14              5   27    6   18   \n",
       "P03              8             7             11   18    2    6   \n",
       "P04              8             6             11   20    1    9   \n",
       "P05             13            10              6   25   14    9   \n",
       "\n",
       "      particpationStartDateTime  \n",
       "pcode                            \n",
       "P01   2019-05-08 00:00:00+09:00  \n",
       "P02   2019-05-08 00:00:00+09:00  \n",
       "P03   2019-05-08 00:00:00+09:00  \n",
       "P04   2019-05-08 00:00:00+09:00  \n",
       "P05   2019-05-08 00:00:00+09:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "PARTICIPANTS = pd.read_csv(PATH_PARTICIPANT).set_index('pcode').assign(\n",
    "    particpationStartDateTime=lambda x: pd.to_datetime(\n",
    "        x['participationStartDate'], format='%Y-%m-%d'\n",
    "    ).dt.tz_localize(DEFAULT_TZ)\n",
    ")\n",
    "PARTICIPANTS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belows are some demographics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- participationStartDate: {'n': 77, 'cardinality': 3, 'value_count': '2019-05-08:27, 2019-05-16:25, 2019-04-30:25'}\n",
      "- age: {'n': 77, 'sum': 1686, 'mean': 21.896103896103895, 'SD': 3.8613619617422406, 'med': 21.0, 'range': (17, 38), 'conf.': (21.01968223607122, 22.77252555613657), 'nan_count': 0}\n",
      "- gender: {'n': 77, 'cardinality': 2, 'value_count': 'M:53, F:24'}\n",
      "- openness: {'n': 77, 'sum': 787, 'mean': 10.220779220779221, 'SD': 2.8956563505732467, 'med': 11.0, 'range': (3, 15), 'conf.': (9.563545847995773, 10.87801259356267), 'nan_count': 0}\n",
      "- conscientiousness: {'n': 77, 'sum': 820, 'mean': 10.64935064935065, 'SD': 2.3662441579221882, 'med': 11.0, 'range': (5, 15), 'conf.': (10.112279104782713, 11.186422193918586), 'nan_count': 0}\n",
      "- neuroticism: {'n': 77, 'sum': 618, 'mean': 8.025974025974026, 'SD': 2.6900108881310953, 'med': 8.0, 'range': (3, 14), 'conf.': (7.4154164477308075, 8.636531604217245), 'nan_count': 0}\n",
      "- extraversion: {'n': 77, 'sum': 703, 'mean': 9.12987012987013, 'SD': 3.0015375417426937, 'med': 9.0, 'range': (3, 15), 'conf.': (8.448604674559745, 9.811135585180514), 'nan_count': 0}\n",
      "- agreeableness: {'n': 77, 'sum': 805, 'mean': 10.454545454545455, 'SD': 2.526415468527935, 'med': 11.0, 'range': (5, 15), 'conf.': (9.881119481845285, 11.027971427245625), 'nan_count': 0}\n",
      "- PSS: {'n': 77, 'sum': 1294, 'mean': 16.805194805194805, 'SD': 7.178254737745983, 'med': 16.0, 'range': (3, 32), 'conf.': (15.175930831569763, 18.434458778819845), 'nan_count': 0}\n",
      "- PHQ: {'n': 77, 'sum': 380, 'mean': 4.935064935064935, 'SD': 4.609308837152732, 'med': 4.0, 'range': (0, 19), 'conf.': (3.888880158116967, 5.981249712012904), 'nan_count': 0}\n",
      "- GHQ: {'n': 77, 'sum': 780, 'mean': 10.12987012987013, 'SD': 5.894579689829796, 'med': 10.0, 'range': (1, 27), 'conf.': (8.791964652957894, 11.467775606782364), 'nan_count': 0}\n",
      "- particpationStartDateTime: {'n': 77, 'cardinality': 3, 'value_count': '2019-05-08 00:00:00+09:00:27, 2019-05-16 00:00:00+09:00:25, 2019-04-30 00:00:00+09:00:25'}\n"
     ]
    }
   ],
   "source": [
    "for c in PARTICIPANTS.columns:\n",
    "    print(f'- {c}:', summary(PARTICIPANTS[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels (via ESM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responseTime</th>\n",
       "      <th>scheduledTime</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>attention</th>\n",
       "      <th>stress</th>\n",
       "      <th>duration</th>\n",
       "      <th>disturbance</th>\n",
       "      <th>change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>1557278103000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>1557278986000</td>\n",
       "      <td>1.557279e+12</td>\n",
       "      <td>-3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>1557281772000</td>\n",
       "      <td>1.557282e+12</td>\n",
       "      <td>-3</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>1557287138000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>1557291117000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        responseTime  scheduledTime  valence  arousal  attention  stress  \\\n",
       "pcode                                                                      \n",
       "P01    1557278103000            NaN        0        0          0      -1   \n",
       "P01    1557278986000   1.557279e+12       -3        3          3       3   \n",
       "P01    1557281772000   1.557282e+12       -3       -2          2       2   \n",
       "P01    1557287138000            NaN        2       -1          2       0   \n",
       "P01    1557291117000            NaN        3        3          3      -3   \n",
       "\n",
       "       duration  disturbance  change  \n",
       "pcode                                 \n",
       "P01        20.0            3      -2  \n",
       "P01         5.0           -1      -3  \n",
       "P01        15.0            3      -2  \n",
       "P01        15.0            1      -1  \n",
       "P01        20.0            1       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "LABELS = pd.read_csv(PATH_ESM).set_index(\n",
    "    ['pcode']\n",
    ")\n",
    "LABELS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- responseTime: {'n': 5582, 'sum': 8694314195328000, 'mean': 1557562557385.8833, 'SD': 590915040.4254278, 'med': 1557562969500.0, 'range': (1556582982000, 1558545246000), 'conf.': (1557547052362.8618, 1557578062408.9048), 'nan_count': 0}\n",
      "- scheduledTime: {'n': 5582, 'sum': 5175814282500000.0, 'mean': 1557572760306.9517, 'SD': 591697484.8543198, 'med': 1557565860000.0, 'range': (1556586120000.0, nan), 'conf.': (1557552635074.4736, 1557592885539.4297), 'nan_count': 2259}\n",
      "- valence: {'n': 5582, 'sum': 3665, 'mean': 0.6565747044070226, 'SD': 1.4184297545899174, 'med': 1.0, 'range': (-3, 3), 'conf.': (0.6193565182132938, 0.6937928906007513), 'nan_count': 0}\n",
      "- arousal: {'n': 5582, 'sum': -529, 'mean': -0.09476890003582945, 'SD': 1.6675313128774563, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.13852326339835566, -0.051014536673303246), 'nan_count': 0}\n",
      "- attention: {'n': 5582, 'sum': 2236, 'mean': 0.4005732712289502, 'SD': 1.6113242733571864, 'med': 1.0, 'range': (-3, 3), 'conf.': (0.3582937246879792, 0.4428528177699212), 'nan_count': 0}\n",
      "- stress: {'n': 5582, 'sum': -1450, 'mean': -0.25976352561805804, 'SD': 1.6154902647587075, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.30215238363050767, -0.21737466760560845), 'nan_count': 0}\n",
      "- duration: {'n': 5582, 'sum': 141955.0, 'mean': 26.390593047034766, 'SD': 18.060980770860386, 'med': 20.0, 'range': (5.0, nan), 'conf.': (25.90782735251826, 26.87335874155127), 'nan_count': 203}\n",
      "- disturbance: {'n': 5582, 'sum': -243, 'mean': -0.04353278394840559, 'SD': 1.7587124884936127, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.0896796506833856, 0.0026140827865744204), 'nan_count': 0}\n",
      "- change: {'n': 5582, 'sum': -52, 'mean': -0.009315657470440702, 'SD': 0.9046571244275675, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.03305296077324875, 0.014421645832367342), 'nan_count': 0}\n"
     ]
    }
   ],
   "source": [
    "for c in LABELS.columns:\n",
    "    print(f'- {c}:', summary(LABELS[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belows are some demographics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- # Inst.: {'n': 77, 'sum': 5582, 'mean': 72.49350649350649, 'SD': 16.02270048911147, 'med': 74.0, 'range': (20, 110), 'conf.': (68.85679957506535, 76.13021341194762), 'nan_count': 0}\n",
      "- # Inst. - Scheduled: {'n': 76, 'sum': 3323, 'mean': 43.723684210526315, 'SD': 19.36291898394835, 'med': 43.5, 'range': (3, 83), 'conf.': (39.29906768289902, 48.14830073815361), 'nan_count': 0}\n",
      "- # Inst. - Voluntary: {'n': 77, 'sum': 2259, 'mean': 29.337662337662337, 'SD': 16.297893300742235, 'med': 27.0, 'range': (2, 74), 'conf.': (25.6384943127028, 33.03683036262187), 'nan_count': 0}\n",
      "- Samp. period: {'n': 5505, 'sum': 42240670.0, 'mean': 7673.146230699364, 'SD': 13193.471538029606, 'med': 3090.0, 'range': (1.0, 136446.0), 'conf.': (7324.548923384188, 8021.743538014541), 'nan_count': 0}\n",
      "- responseTime: {'n': 5582, 'sum': 8694314195328000, 'mean': 1557562557385.8833, 'SD': 590915040.4254278, 'med': 1557562969500.0, 'range': (1556582982000, 1558545246000), 'conf.': (1557547052362.8618, 1557578062408.9048), 'nan_count': 0}\n",
      "- scheduledTime: {'n': 5582, 'sum': 5175814282500000.0, 'mean': 1557572760306.9517, 'SD': 591697484.8543198, 'med': 1557565860000.0, 'range': (1556586120000.0, nan), 'conf.': (1557552635074.4736, 1557592885539.4297), 'nan_count': 2259}\n",
      "- valence: {'n': 5582, 'sum': 3665, 'mean': 0.6565747044070226, 'SD': 1.4184297545899174, 'med': 1.0, 'range': (-3, 3), 'conf.': (0.6193565182132938, 0.6937928906007513), 'nan_count': 0}\n",
      "- arousal: {'n': 5582, 'sum': -529, 'mean': -0.09476890003582945, 'SD': 1.6675313128774563, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.13852326339835566, -0.051014536673303246), 'nan_count': 0}\n",
      "- attention: {'n': 5582, 'sum': 2236, 'mean': 0.4005732712289502, 'SD': 1.6113242733571864, 'med': 1.0, 'range': (-3, 3), 'conf.': (0.3582937246879792, 0.4428528177699212), 'nan_count': 0}\n",
      "- stress: {'n': 5582, 'sum': -1450, 'mean': -0.25976352561805804, 'SD': 1.6154902647587075, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.30215238363050767, -0.21737466760560845), 'nan_count': 0}\n",
      "- duration: {'n': 5582, 'sum': 141955.0, 'mean': 26.390593047034766, 'SD': 18.060980770860386, 'med': 20.0, 'range': (5.0, nan), 'conf.': (25.90782735251826, 26.87335874155127), 'nan_count': 203}\n",
      "- disturbance: {'n': 5582, 'sum': -243, 'mean': -0.04353278394840559, 'SD': 1.7587124884936127, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.0896796506833856, 0.0026140827865744204), 'nan_count': 0}\n",
      "- change: {'n': 5582, 'sum': -52, 'mean': -0.009315657470440702, 'SD': 0.9046571244275675, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.03305296077324875, 0.014421645832367342), 'nan_count': 0}\n"
     ]
    }
   ],
   "source": [
    "inst = LABELS.groupby('pcode').count().iloc[:, -1]\n",
    "inst_sch = LABELS.loc[lambda x: ~x['scheduledTime'].isna(), :].groupby('pcode').count().iloc[:, -1]\n",
    "inst_vol = LABELS.loc[lambda x: x['scheduledTime'].isna(), :].groupby('pcode').count().iloc[:, -1]\n",
    "resp_time = LABELS.assign(\n",
    "    timestamp=lambda x: pd.to_datetime(x['responseTime'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ)\n",
    ")\n",
    "sam = np.concatenate([\n",
    "    (resp_time.loc[p, 'timestamp'].array - resp_time.loc[p, 'timestamp'].array.shift(1)).dropna().total_seconds()\n",
    "    for p in LABELS.index.unique()\n",
    "])\n",
    "\n",
    "print('- # Inst.:', summary(inst))\n",
    "print('- # Inst. - Scheduled:', summary(inst_sch))\n",
    "print('- # Inst. - Voluntary:', summary(inst_vol))\n",
    "print('- Samp. period:', summary(sam))\n",
    "for c in LABELS.columns:\n",
    "    print(f'- {c}:', summary(LABELS[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LABELS.loc[\n",
    "    :, lambda x: ~x.columns.isin(['responseTime', 'scheduledTime'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/rpy2/robjects/pandas2ri.py:56: UserWarning: DataFrame contains duplicated elements in the index, which will lead to loss of the row names in the resulting data.frame\n",
      "  warnings.warn('DataFrame contains duplicated elements in the index, '\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i data -w 16 -h 6 -u cm\n",
    "\n",
    "data <- data %>% pivot_longer(\n",
    "    cols = c('valence', 'arousal', 'attention', 'stress', 'duration', 'disturbance', 'change'),\n",
    "    names_to = 'metric'\n",
    ")\n",
    "\n",
    "p_rest <- ggplot(\n",
    "    data %>% filter(metric != 'duration'), aes(x=metric, y=value)\n",
    ") + geom_boxplot(\n",
    ") + geom_point(\n",
    "    data = data %>% filter(\n",
    "        metric != 'duration'\n",
    "    ) %>% group_by(\n",
    "        metric\n",
    "    ) %>% summarise(\n",
    "        mean = mean(value, na.rm=TRUE)\n",
    "    ),\n",
    "    mapping=aes(x=metric, y=mean),\n",
    "    shape=21,\n",
    "    stroke=1,\n",
    "    size=2,\n",
    "    fill='white'\n",
    ") + scale_x_discrete(\n",
    "    name=NULL,\n",
    "    limits=c('valence', 'arousal', 'stress', 'attention', 'disturbance', 'change'),\n",
    "    labels=c('Valence', 'Arousal', 'Stress', 'Attent.', 'Disturb.', 'Change'),\n",
    ") + scale_y_continuous(\n",
    "    name='Response',\n",
    "    breaks=-3:3\n",
    ") + THEME_DEFAULT\n",
    "\n",
    "p_duration <- ggplot(\n",
    "    data %>% filter(metric == 'duration'), aes(x=metric, y=value)\n",
    ") + geom_boxplot(\n",
    ") + geom_point(\n",
    "    data = data %>% filter(\n",
    "        metric == 'duration'\n",
    "    ) %>% group_by(\n",
    "        metric\n",
    "    ) %>% summarise(\n",
    "        mean = mean(value, na.rm=TRUE)\n",
    "    ),\n",
    "    mapping=aes(x=metric, y=mean),\n",
    "    shape=21,\n",
    "    stroke=1,\n",
    "    size=2,\n",
    "    fill='white'\n",
    ")+ scale_x_discrete(\n",
    "    name=NULL,\n",
    "    limits=c('duration'),\n",
    "    labels=c('Duration'),\n",
    ") + scale_y_continuous(\n",
    "    name=NULL,\n",
    "    breaks=seq(from=5, to=60, by=10)\n",
    ") + THEME_DEFAULT\n",
    "\n",
    "p <- p_rest + p_duration + plot_layout(widths=c(4, 0.8))\n",
    "ggsave('./fig/dist-labels.pdf', plot=p, width=16, height=6, unit='cm', device=cairo_pdf)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each participant reported their labels multiple times (i.e., repeated measure), repeated measure correlation between affect labels were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LABELS.reset_index()[[\n",
    "    'pcode', 'valence', 'arousal', 'stress', 'attention', 'disturbance', 'change'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valence - arousal : R = 0.3858505 (p = 6.005105e-195 ) \n",
      "valence - stress : R = -0.5918317 (p = 0 ) \n",
      "valence - attention : R = 0.2880379 (p = 1.138238e-105 ) \n",
      "valence - disturbance : R = -0.02946216 (p = 0.02880521 ) \n",
      "valence - change : R = 0.3163849 (p = 2.978458e-128 ) \n",
      "arousal - stress : R = -0.2020498 (p = 8.095612e-52 ) \n",
      "arousal - attention : R = 0.4354836 (p = 1.162341e-253 ) \n",
      "arousal - disturbance : R = 0.0284022 (p = 0.03507786 ) \n",
      "arousal - change : R = 0.1673468 (p = 7.1899e-36 ) \n",
      "stress - attention : R = -0.1515681 (p = 1.176714e-29 ) \n",
      "stress - disturbance : R = 0.08679844 (p = 1.108432e-10 ) \n",
      "stress - change : R = -0.2907816 (p = 9.560751e-108 ) \n",
      "attention - disturbance : R = 0.1182107 (p = 1.363257e-18 ) \n",
      "attention - change : R = 0.1163492 (p = 4.682081e-18 ) \n",
      "disturbance - change : R = -0.2216444 (p = 3.009815e-62 ) \n"
     ]
    }
   ],
   "source": [
    "%%R -i data \n",
    "\n",
    "com <- combn(c('valence', 'arousal', 'stress', 'attention', 'disturbance', 'change'), 2)\n",
    "\n",
    "for(i in 1:ncol(com)) {\n",
    "    a <- com[, i][1]\n",
    "    b <- com[, i][2]\n",
    "    r <- rmcorr(participant = 'pcode', measure1=a, measure2=b, dataset=data)\n",
    "    cat(a, '-', b, ': R =', r$r, '(p =', r$p, ') \\n')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def _load_data(\n",
    "    name: str\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    paths = [\n",
    "        (d, os.path.join(PATH_SENSOR, d, f'{name}.csv'))\n",
    "        for d in os.listdir(PATH_SENSOR)\n",
    "        if d.startswith('P')\n",
    "    ]\n",
    "    return pd.concat(\n",
    "        filter(\n",
    "            lambda x: len(x.index), \n",
    "            [\n",
    "                pd.read_csv(p).assign(pcode=pcode)\n",
    "                for pcode, p in paths\n",
    "                if os.path.exists(p)\n",
    "            ]\n",
    "        ), ignore_index=True\n",
    "    ).assign(\n",
    "        timestamp=lambda x: pd.to_datetime(x['timestamp'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ)\n",
    "    ).set_index(\n",
    "        ['pcode', 'timestamp']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from datetime import timedelta as td\n",
    "\n",
    "\n",
    "STATS = []\n",
    "\n",
    "for data_type in DATA_TYPES:\n",
    "    dat = _load_data(data_type)\n",
    "    inst = dat.groupby('pcode').count().iloc[:, -1]\n",
    "    samp = np.concatenate([\n",
    "        (dat.loc[(p,), :].index.array - dat.loc[(p,), :].index.array.shift(1)).dropna().total_seconds()\n",
    "        for p in dat.index.get_level_values('pcode').unique()\n",
    "    ])\n",
    "    inst, samp = summary(inst), summary(samp)\n",
    "    \n",
    "    print('#'*5, data_type, '#'*5)\n",
    "    print('- # Inst.:', inst)\n",
    "    print('- Samp. period:', samp)\n",
    "    \n",
    "    for c in dat.columns:\n",
    "        print(f'- {c}:', summary(dat[c]))\n",
    "        \n",
    "    del dat\n",
    "    gc.collect()\n",
    "    \n",
    "STATS = pd.DataFrame(STATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label\n",
    "\n",
    "Because we intended to collected participants' responses to ESMs not voluntary responses, we screend out some responses as follows:\n",
    "* We first screen out ESM responses that does not have 'scheduledTime' (meaning that a given ESM was expired or participants voluntarily reported their affective states regardless of ESM delivery). \n",
    "* Since we will evaluate our model using LOSO, the small number of responses for each participant might lead to inappropriate performance evaluation. We emprically set the number of the minimum responses upon ESM delivery as 5 per day (i.e., a half of our guides), so that we excluded participants whose responses to ESM less than 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LABELS_VALID = LABELS.loc[\n",
    "    lambda x: ~x['scheduledTime'].isna(), :\n",
    "]\n",
    "print(f'# Non-voluntary response: {len(LABELS_VALID)}')\n",
    "print(summary(LABELS_VALID.groupby('pcode').count().iloc[:, -1]))\n",
    "\n",
    "excl_pcode = LABELS_VALID.loc[\n",
    "    lambda x: ~x['scheduledTime'].isna()\n",
    "].groupby('pcode').count().iloc[:, -1].loc[lambda y: y < 35]\n",
    "\n",
    "LABELS_VALID = LABELS_VALID.loc[\n",
    "    lambda x:  ~x.index.get_level_values('pcode').isin(excl_pcode.index), :\n",
    "]\n",
    "print(f'# Response from participants with enough responses: {len(LABELS_VALID)}')\n",
    "print(summary(LABELS_VALID.groupby('pcode').count().iloc[:, -1]))\n",
    "\n",
    "print('# Participants whose responses to ESM delivery were less then 35')\n",
    "print(excl_pcode, f'#participants = {len(excl_pcode)} / #response = {sum(excl_pcode)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider binary classifications for valence, arousal, stress, and disturbance, in which a label value greater than 0 is \"HIGH\" (1) and the rest is \"LOW\" (0), at the arrival of ESM prompts (*scheduledTime*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "LABELS_PROC = (\n",
    "    LABELS_VALID\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        timestamp=lambda x: pd.to_datetime(x['scheduledTime'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ),\n",
    "        attention_bin=lambda x: np.where(x['attention'] > 0, 1, 0)\n",
    "    )\n",
    "    .loc[:, ['pcode', 'timestamp', 'attention', 'attention_bin']]  # attention: 연속형, attention_bin: 이진분류된 데이터\n",
    "    .set_index(['pcode', 'timestamp'])\n",
    ")\n",
    "\n",
    "LABELS_PROC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "inst = LABELS_PROC.groupby('pcode').count().iloc[:, -1]\n",
    "for c in [c for c in LABELS_PROC.columns if c.endswith('_bin')]:\n",
    "    print(f'- {c}:', summary(LABELS_PROC[c].astype(object)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each type of sensor data, we applied different preprocessing. Detailed decsription is provided in the paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as dist\n",
    "from typing import Dict, Union\n",
    "import pygeohash as geo\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "# SkinTemperature.csv\n",
    "def _proc_skin_temperature(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['temperature'].astype('float32')\n",
    "\n",
    "\n",
    "# RRI.csv\n",
    "def _proc_rri(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['interval'].astype('float32')\n",
    "\n",
    "\n",
    "# HR.csv\n",
    "def _proc_hr(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    return data['bpm'].astype('float32')\n",
    "    \n",
    "\n",
    "# EDA.csv\n",
    "\n",
    "def _proc_eda(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    resistance = data['resistance'].astype('float32')\n",
    "\n",
    "    # log변환 + 0을 엡실론으로\n",
    "    epsilon = 1e-3\n",
    "    log_resistance = np.log(resistance + epsilon)\n",
    "\n",
    "    # 이상치 탐지\n",
    "    z = zscore(log_resistance)\n",
    "\n",
    "    # 이상치 NaN으로 → 보간\n",
    "    log_resistance[z > 3] = np.nan\n",
    "    log_resistance = log_resistance.interpolate(method='linear')\n",
    "\n",
    "    # 기존 코드와 이름, 타입 일치\n",
    "    log_resistance.name = 'resistance'\n",
    "    return log_resistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from functools import reduce\n",
    "\n",
    "FUNC_PROC = {\n",
    "    'EDA': _proc_eda,\n",
    "    'HR': _proc_hr,\n",
    "    'RRI': _proc_rri,\n",
    "    'SkinTemperature': _proc_skin_temperature,\n",
    "}\n",
    "\n",
    "\n",
    "def _process(data_type: str):\n",
    "    log(f'Begin to processing data: {data_type}')\n",
    "    \n",
    "    abbrev = DATA_TYPES[data_type]\n",
    "    data_raw = _load_data(data_type)\n",
    "    data_proc = FUNC_PROC[data_type](data_raw)\n",
    "    result = dict()\n",
    "    \n",
    "    if type(data_proc) is dict:\n",
    "        for k, v in data_proc.items():\n",
    "            result[f'{abbrev}_{k}'] = v\n",
    "    else:\n",
    "        result[abbrev] = data_proc\n",
    "        \n",
    "    log(f'Complete processing data: {data_type}')\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "with on_ray(num_cpus=12):\n",
    "    jobs = []\n",
    "    \n",
    "    func = ray.remote(_process).remote\n",
    "    \n",
    "    for data_type in DATA_TYPES:\n",
    "        job = func(data_type)\n",
    "        jobs.append(job)\n",
    "\n",
    "    jobs = ray.get(jobs)\n",
    "    jobs = reduce(lambda a, b: {**a, **b}, jobs)\n",
    "    dump(jobs, os.path.join(PATH_INTERMEDIATE, 'proc.pkl'))\n",
    "\n",
    "    del jobs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "DATA = load(os.path.join(PATH_INTERMEDIATE, 'proc.pkl'))\n",
    "#categorial, numeric 변수 수 계산\n",
    "N_NUMERIC, N_CATEGORICAL = 0, 0\n",
    "\n",
    "for k, v in DATA.items():\n",
    "    if v.dtype.kind.isupper() or v.dtype.kind == 'b': \n",
    "        N_CATEGORICAL = N_CATEGORICAL + 1\n",
    "    else:\n",
    "        N_NUMERIC = N_NUMERIC + 1\n",
    "        \n",
    "    inst = v.groupby('pcode').count()\n",
    "    sam = np.concatenate([\n",
    "        (v.loc[(p,)].index.array - v.loc[(p,)].index.array.shift(1)).dropna().total_seconds()\n",
    "        for p in v.index.get_level_values('pcode').unique()\n",
    "    ])\n",
    "    \n",
    "    print('#'*5, k, '#'*5, )\n",
    "    print('- # Inst.:', summary(inst))\n",
    "    print('- Samp. period:', summary(sam))\n",
    "    print('- Values', summary(v))\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "print(f'# categorical data: {N_CATEGORICAL}/# numeric data: {N_NUMERIC}')\n",
    "del DATA\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Callable, Union, Tuple, List, Optional, Iterable\n",
    "from datetime import timedelta as td\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "import ray\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# _extract: 한 명의 참가자로부터 feature 추출\n",
    "# data, label(timestamp별 정답값), label_values(라벨 클래스 목록), window_data, window_label, categories, const_features, resample\n",
    "# time window별 통합: 수치형 센서 - mean, std, skewness, kurtosis, median, 시계열 복잡도, 변화량 절댓값 합, 히스토그램 기반 엔트로피\n",
    "# 시간대별 패턴: 비정기적 설문조사 주기에 맞게 하루를 7개의 시간대로 분할, one-hot encoding \n",
    "# 최종 feature 예시: ESM#LIK#H24\n",
    "# extract: 여러 참가자에게 _extract함수 -> 통합\n",
    "\n",
    "# 최종 학습 데이터 형태: X(각 참가자, 각 시점에 대한 Feature, DataFrame), y(label, ndarray), group(LOSO시 각 행에 속한 참가자 ID, ndarray), t_norm(응답시간 timestamp를 정규화한 시간 정보), date_times(각 데이터 포인트으ㅢ 실제 timestamp)\n",
    "# X: PIF#AGE, ACC#AVG#H06, APP_CAT#DUR=COMMUNICATION#H01, ESM#HRN=MORNING  . . .\n",
    "# y: np.array([1, 0, 1, 1  . . .\n",
    "# group:  np.array(['P01', 'P01', 'P02' . . .\n",
    "# t_norm: np.array([0.0, 10800.0, 3600.0, 7200.0 . . .\n",
    "# date_times: np.array([\n",
    "#     Timestamp('2025-05-01 10:00:00'),\n",
    "#     Timestamp('2025-05-01 13:00:00'),\n",
    "#     Timestamp('2025-05-01 11:00:00'), . . .\n",
    "\n",
    "def _safe_na_check(_v):\n",
    "    _is_nan_inf = False\n",
    "    try:\n",
    "        _is_nan_inf = np.isnan(_v) or np.isinf(_v)\n",
    "    except:\n",
    "        _is_nan_inf = False\n",
    "    return _is_nan_inf or _v is None\n",
    "\n",
    "\n",
    "def _extract(\n",
    "        pid: str,\n",
    "        data: Dict[str, pd.Series],\n",
    "        label: pd.Series,\n",
    "        label_values: List[str],\n",
    "        window_data: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "        window_label: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],        \n",
    "        categories: Dict[str, Optional[List[any]]] = None,        \n",
    "        resample_s: Dict[str, float] = None\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    _s = time.time()\n",
    "    log(f\"Begin feature extraction on {pid}'s data.\")\n",
    "\n",
    "    categories = categories or dict()\n",
    "    resample_s = resample_s or dict()\n",
    "\n",
    "    X, y, date_times = [], [], []\n",
    " \n",
    "    for timestamp in label.index:\n",
    "        row = dict()\n",
    "\n",
    "        label_cur = label.at[timestamp]\n",
    "        t = timestamp - td(milliseconds=1)\n",
    "\n",
    "        # Features from sensor data\n",
    "        for d_key, d_val in data.items():\n",
    "            is_numeric = d_key not in categories\n",
    "            cats = categories.get(d_key) or list()\n",
    "            d_val = d_val.sort_index()\n",
    "\n",
    "            if is_numeric or cats:\n",
    "                try:\n",
    "                    v = d_val.loc[:t].iloc[-1]\n",
    "                except (KeyError, IndexError):\n",
    "                    v = 0\n",
    "\n",
    "                if is_numeric:\n",
    "                    row[f'{d_key}#VAL'] = v\n",
    "                else:\n",
    "                    for c in cats:\n",
    "                        row[f'{d_key}#VAL={c}'] = v == c\n",
    "\n",
    "            # catogorial 데이터의 최근 상태 변화 시간\n",
    "            if not is_numeric:\n",
    "                try:\n",
    "                    v = d_val.loc[:t]\n",
    "                    row[f'{d_key}#DSC'] = (t - v.index[-1]).total_seconds() if len(v) else -1.0\n",
    "                    for c in cats:\n",
    "                        v_sub = v.loc[lambda x: x == c].index\n",
    "                        row[f'{d_key}#DSC={c}'] = (t - v_sub[-1]).total_seconds() if len(v_sub) else -1.0\n",
    "                except (KeyError, IndexError):\n",
    "                    row[f'{d_key}#DSC'] = -1.0\n",
    "                    for c in cats:\n",
    "                        row[f'{d_key}#DSC={c}'] = -1.0\n",
    "\n",
    "            # Time-window 기반 피처 (resampling 포함)\n",
    "            sample_rate = resample_s.get(d_key) or 1\n",
    "            d_val_res = d_val.resample(f'{sample_rate}S', origin='start')\n",
    "            if is_numeric:\n",
    "                \"\"\" 보간 방식 추가 부분 \"\"\"\n",
    "                if d_key == \"interval\":\n",
    "                    interval_series = d_val.dropna()\n",
    "                    if len(interval_series) >= 4:\n",
    "                        ts = interval_series.index.view(np.int64) // 10**6\n",
    "                        cs = CubicSpline(ts, interval_series.values)\n",
    "                        full_ts = d_val.index.view(np.int64) // 10**6\n",
    "                        d_val[:] = cs(full_ts)\n",
    "                else: \n",
    "                    d_val_res = d_val_res.mean().interpolate(method='linear').dropna()\n",
    "            else:\n",
    "                d_val_res = d_val_res.ffill().dropna()\n",
    "\n",
    "            for w_key, w_val in window_data.items():\n",
    "                w_val = w_val(t) if isinstance(w_val, Callable) else w_val\n",
    "                try:\n",
    "                    v = d_val_res.loc[t - td(seconds=w_val):t]\n",
    "                    # numeric 데이터일 경우에만 변환\n",
    "                    if is_numeric:\n",
    "                        v_arr = v.values.astype(np.float64)\n",
    "                    else:\n",
    "                        v_arr = v\n",
    "                except (KeyError, IndexError):\n",
    "                    continue\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore')\n",
    "\n",
    "                    if is_numeric:\n",
    "                        hist, _ = np.histogram(v, bins='doane', density=False)\n",
    "                        std = np.sqrt(np.var(v, ddof=1)) if len(v) > 1 else 0\n",
    "                        v_norm = (v - np.mean(v)) / std if std != 0 else np.zeros(len(v))\n",
    "\n",
    "                        row[f'{d_key}#AVG#{w_key}'] = np.float32(np.mean(v_arr))\n",
    "                        row[f'{d_key}#STD#{w_key}'] = np.float32(std)\n",
    "                        row[f'{d_key}#SKW#{w_key}'] = np.float32(stats.skew(v_arr, bias=False))\n",
    "                        row[f'{d_key}#KUR#{w_key}'] = np.float32(stats.kurtosis(v_arr, bias=False))\n",
    "                        row[f'{d_key}#ASC#{w_key}'] = np.float32(np.sum(np.abs(np.diff(v_arr))))\n",
    "                        row[f'{d_key}#BEP#{w_key}'] = np.float32(stats.entropy(hist))\n",
    "                        row[f'{d_key}#MED#{w_key}'] = np.float32(np.median(v_arr))\n",
    "                        row[f'{d_key}#TSC#{w_key}'] = np.float32(np.sqrt(np.sum(np.power(np.diff(v_norm), 2))))\n",
    "                    else:\n",
    "                        cnt = v.value_counts()\n",
    "                        val, sup = cnt.index, cnt.values\n",
    "                        hist = {k: v for k, v in zip(val, sup)}\n",
    "\n",
    "                        row[f'{d_key}#ETP#{w_key}'] = stats.entropy(sup / len(v))\n",
    "                        row[f'{d_key}#ASC#{w_key}'] = np.sum(v.values[1:] != v.values[:-1])\n",
    "\n",
    "                        if len(cats) == 2:\n",
    "                            c = cats[0]\n",
    "                            row[f'{d_key}#DUR#{w_key}'] = hist[c] / len(v) if c in hist else 0\n",
    "                        else:\n",
    "                            for c in cats:\n",
    "                                row[f'{d_key}#DUR={c}#{w_key}'] = hist[c] / len(v) if c in hist else 0\n",
    "\n",
    "        # 시간 기반 피처\n",
    "        day_of_week = ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN'][t.isoweekday() - 1]\n",
    "        is_weekend = 'Y' if t.isoweekday() > 5 else 'N'\n",
    "        hour = t.hour\n",
    "\n",
    "        if 6 <= hour < 9:\n",
    "            hour_name = 'DAWN'\n",
    "        elif 9 <= hour < 12:\n",
    "            hour_name = 'MORNING'\n",
    "        elif 12 <= hour < 15:\n",
    "            hour_name = 'AFTERNOON'\n",
    "        elif 15 <= hour < 18:\n",
    "            hour_name = 'LATE_AFTERNOON'\n",
    "        elif 18 <= hour < 21:\n",
    "            hour_name = 'EVENING'\n",
    "        elif 21 <= hour < 24:\n",
    "            hour_name = 'NIGHT'\n",
    "        else:\n",
    "            hour_name = 'MIDNIGHT'\n",
    "\n",
    "        for d in ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN']:\n",
    "            row[f'ESM#DOW={d}'] = d == day_of_week\n",
    "\n",
    "        for d in ['Y', 'N']:\n",
    "            row[f'ESM#WKD={d}'] = d == is_weekend\n",
    "\n",
    "        for d in ['DAWN', 'MORNING', 'AFTERNOON', 'LATE_AFTERNOON', 'EVENING', 'NIGHT', 'MIDNIGHT']:\n",
    "            row[f'ESM#HRN={d}'] = d == hour_name\n",
    "\n",
    "        # 응답 이력 기반 피처\n",
    "        for w_key, w_val in window_label.items():\n",
    "            w_val = w_val(t) if isinstance(w_val, Callable) else w_val\n",
    "            try:\n",
    "                v = label.loc[t - td(seconds=w_val):t]\n",
    "                if len(label_values) <= 2:\n",
    "                    row[f'ESM#LIK#{w_key}'] = np.sum(v == label_values[0]) / len(v) if len(v) > 0 else 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK={l}#{w_key}'] = np.sum(v == l) / len(v) if len(v) > 0 else 0\n",
    "            except (KeyError, IndexError):\n",
    "                if len(label_values) <= 2: \n",
    "                    row[f'ESM#LIK#{w_key}'] = 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK={l}#{w_key}'] = 0\n",
    "\n",
    "        row = {\n",
    "            k: 0.0 if _safe_na_check(v) else v\n",
    "            for k, v in row.items()\n",
    "        }\n",
    "        X.append(row)\n",
    "        y.append(label_cur)\n",
    "        date_times.append(timestamp)\n",
    "\n",
    "    log(f\"Complete feature extraction on {pid}'s data ({time.time() - _s:.2f} s).\")\n",
    "    X, y, group, date_times = pd.DataFrame(X), np.asarray(y), np.repeat(pid, len(y)), np.asarray(date_times)\n",
    "    return X, y, group, date_times\n",
    "\n",
    "\n",
    "def extract(\n",
    "        pids: Iterable[str],\n",
    "        data: Dict[str, pd.Series],\n",
    "        label: pd.Series,\n",
    "        label_values: List[str],\n",
    "        window_data: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "        window_label: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],        \n",
    "        categories: Dict[str, Optional[List[any]]] = None,        \n",
    "        resample_s: Dict[str, float] = None,\n",
    "        with_ray: bool=False\n",
    "):\n",
    "    if with_ray and not ray.is_initialized():\n",
    "        raise EnvironmentError('Ray should be initialized if \"with_ray\" is set as True.')\n",
    "\n",
    "    func = ray.remote(_extract).remote if with_ray else _extract\n",
    "    jobs = []\n",
    "\n",
    "    for pid in pids:\n",
    "        d = dict()\n",
    "        for k, v in data.items():\n",
    "            try:\n",
    "                d[k] = v.loc[(pid, )]\n",
    "            except (KeyError, IndexError):\n",
    "                pass\n",
    "\n",
    "        job = func(\n",
    "            pid=pid,\n",
    "            data=d,\n",
    "            label=label.loc[(pid, )],\n",
    "            label_values=label_values,\n",
    "            window_data=window_data,\n",
    "            window_label=window_label,\n",
    "            categories=categories,\n",
    "            resample_s=resample_s\n",
    "        )\n",
    "        jobs.append(job)\n",
    "\n",
    "    jobs = ray.get(jobs) if with_ray else jobs\n",
    "\n",
    "    X = pd.concat([x for x, _, _, _ in jobs], axis=0, ignore_index=True)\n",
    "    y = np.concatenate([x for _, x, _, _ in jobs], axis=0)\n",
    "    group = np.concatenate([x for _, _, x, _ in jobs], axis=0)\n",
    "    date_times = np.concatenate([x for _, _, _, x in jobs], axis=0)\n",
    "\n",
    "    t_s = date_times.min().normalize().timestamp()\n",
    "    t_norm = np.asarray(list(map(lambda x: x.timestamp() - t_s, date_times)))\n",
    "\n",
    "    C, DTYPE = X.columns, X.dtypes\n",
    "\n",
    "    X = X.fillna({\n",
    "        **{c: False for c in C[(DTYPE == object) | (DTYPE == bool)]},\n",
    "        **{c: 0.0 for c in C[(DTYPE != object) & (DTYPE != bool)]},\n",
    "    }).astype({\n",
    "        **{c: 'bool' for c in C[(DTYPE == object) | (DTYPE == bool)]},\n",
    "        **{c: 'float32' for c in C[(DTYPE != object) & (DTYPE != bool)]},\n",
    "    })\n",
    "\n",
    "    return X, y, group, t_norm, date_times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_VALUES = [1, 0]\n",
    "\n",
    "WINDOW_DATA = {\n",
    "    'S30': 30,\n",
    "    'M01': 60,\n",
    "    'M05': 60 * 5,\n",
    "    'M10': 60 * 10,\n",
    "    'M30': 60 * 30,\n",
    "    'H01': 60 * 60,\n",
    "    'H03': 60 * 60 * 3,\n",
    "    'H06': 60 * 60 * 6\n",
    "}\n",
    "\n",
    "WINDOW_LABEL = {\n",
    "    'H06': 60 * 60 * 6,\n",
    "    'H12': 60 * 60 * 12,\n",
    "    'H24': 60 * 60 * 24,\n",
    "}\n",
    "\n",
    "RESAMPLE_s = {\n",
    "    'EDA': 0.5,\n",
    "}\n",
    "\n",
    "DATA = load(os.path.join(PATH_INTERMEDIATE, 'proc.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with on_ray(num_cpus=12):\n",
    "    l = 'attention'\n",
    "\n",
    "    labels = LABELS_PROC[f'{l}_bin']\n",
    "    pids = labels.index.get_level_values('pcode').unique()\n",
    "\n",
    "    feat = extract(\n",
    "        pids=pids, \n",
    "        data=DATA,\n",
    "        label=labels,\n",
    "        label_values=LABEL_VALUES,\n",
    "        window_data=WINDOW_DATA,\n",
    "        window_label=WINDOW_LABEL,\n",
    "        resample_s=RESAMPLE_s,\n",
    "        with_ray=True\n",
    "    )\n",
    "\n",
    "    dump(feat, os.path.join(PATH_INTERMEDIATE, f'{l}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# attention만 대상\n",
    "X, y, group, t, _ = load(os.path.join(PATH_INTERMEDIATE, 'attention.pkl'))\n",
    "\n",
    "print(f'# attention')\n",
    "print(f'- Feature space: {len(X.dtypes)}; Cat.: {np.sum(X.dtypes == bool)}; Num.: {np.sum(X.dtypes != bool)}')\n",
    "print(f'- Label distribution: {np.unique(y, return_counts=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the number of features is same as intented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NUM, N_CAT_B, N_CAT_NB = 0, 0, 0 \n",
    "\n",
    "for k, v in DATA.items():\n",
    "    N_NUM = N_NUM + 1\n",
    "\n",
    "# Features relavant to delivery time\n",
    "N_TIM = 7 + 2 + 7\n",
    "print(f'N_TIM: {N_TIM}')\n",
    "        \n",
    "# Features relevant to latest value\n",
    "N_VAL_NUM = N_NUM\n",
    "print(f'(N_VAL_NUM: {N_VAL_NUM})')\n",
    "\n",
    "# Features from time-windows\n",
    "N_WIN_NUM = N_NUM * 8 * len(WINDOW_DATA)\n",
    "\n",
    "print(f'N_WIN_NUM: {N_WIN_NUM}')\n",
    "\n",
    "\n",
    "# Features from previous labels\n",
    "N_LBL = len(WINDOW_LABEL) * (1 if len(LABEL_VALUES) <= 2 else len(LABEL_VALUES))\n",
    "print(f'N_LBL: {N_LBL}')\n",
    "\n",
    "N_FEAT = N_TIM + N_WIN_NUM + N_LBL\n",
    "print(f'N_FEAT: {N_FEAT}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, features are extracted as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import time\n",
    "import ray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    normalize: bool = False,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    categories: Union[List, Dict[str, Dict[int, str]]] = None\n",
    "):\n",
    "    @contextmanager\n",
    "    def _log(task_type: str):\n",
    "        log(f'In progress: {task_type}.')\n",
    "        _t = time.time()\n",
    "        _err = None\n",
    "        _result = dict()\n",
    "        \n",
    "        try:\n",
    "            yield _result\n",
    "        except:\n",
    "            _err = tb.format_exc()\n",
    "        finally:\n",
    "            _e = time.time() - _t\n",
    "            if _err:\n",
    "                _msg = f'Failure: {task_type} ({_e:.2f}s). Keep running without this task. Caused by: \\n{_err}' \n",
    "            else:\n",
    "                _msg = f'Success: {task_type} ({_e:.2f}s).' \n",
    "                if _result:\n",
    "                    _r = '\\n'.join([f'- {k}: {v}' for k, v in _result.items()])\n",
    "                    _msg = f'{_msg}\\n{_r}'\n",
    "            log(_msg)\n",
    "    \n",
    "    if normalize:\n",
    "        with _log(f'[{name}] Normalizing numeric features'):\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "         \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "           \n",
    "    if select:\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "            \n",
    "        for i, s in enumerate(select):\n",
    "            with _log(f'[{name}] {i+1}-th Feature selection') as r:\n",
    "                C = np.asarray(X_train.columns)\n",
    "                r['# Orig. Feat.'] = f'{len(C)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                r['# Sel. Feat.'] = f'{len(C_sel)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "\n",
    "    if oversample:\n",
    "        with _log(f'[{name}] Oversampling') as r:\n",
    "            if len(C_cat):\n",
    "                M = np.isin(X_train.columns, C_cat)\n",
    "                sampler = SMOTENC(categorical_features=M, random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    with _log(f'[{name}] Training'):\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "        result = FoldResult(\n",
    "            name=name,\n",
    "            estimator=estimator,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            categories=categories\n",
    "        )\n",
    "        dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    \n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            categories=categories,\n",
    "            estimator=clone(estimator),\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Modification on XGBClassifer\n",
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, our feature data has a big-$p$, little-$N$ problem: # sample = 2,619 while # features = 3,356.\n",
    "Therefore, we need to choose important features only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_DUMMY = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "    estimator=LinearSVC(\n",
    "        penalty='l1',\n",
    "        loss='squared_hinge',\n",
    "        dual=False,\n",
    "        tol=1e-3,\n",
    "        C=1e-2,\n",
    "        max_iter=5000,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    threshold=1e-5\n",
    ")\n",
    "\n",
    "CLS = ['attention']\n",
    "SETTINGS = [\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_DUMMY),\n",
    "        oversample=False,\n",
    "        select=None,\n",
    "        name='dummy'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_ns'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_os'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_ns'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_os'\n",
    "    )\n",
    "]\n",
    "\n",
    "with on_ray(num_cpus=16):\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):\n",
    "        p = os.path.join(PATH_INTERMEDIATE, f'{l}.pkl')\n",
    "        par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', l)\n",
    "        os.makedirs(par_dir, exist_ok=True)\n",
    "        \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups,\n",
    "            path=par_dir,\n",
    "            categories=cats,\n",
    "            normalize=True,\n",
    "            split='logo',\n",
    "            random_state=RANDOM_STATE,\n",
    "            **s\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "from itertools import product\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, \\\n",
    "    confusion_matrix, precision_recall_fscore_support, \\\n",
    "    roc_auc_score, matthews_corrcoef, average_precision_score, \\\n",
    "    log_loss, brier_score_loss\n",
    "import scipy.stats.mstats as ms\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_proba: np.ndarray,\n",
    "    classes: np.ndarray\n",
    ") -> Dict[str, any]:\n",
    "    R = {}\n",
    "    n_classes = len(classes)\n",
    "    is_multiclass = n_classes > 2\n",
    "    is_same_y = len(np.unique(y_true)) == 1\n",
    "    R['inst'] = len(y_true)\n",
    "    \n",
    "    for c in classes:\n",
    "        R[f'inst_{c}'] = np.sum(y_true == c)\n",
    "        \n",
    "    if not is_multiclass:\n",
    "        _, cnt = np.unique(y_true, return_counts=True)\n",
    "        \n",
    "        if len(cnt) > 1:\n",
    "            R['class_ratio'] = cnt[0] / cnt[1]\n",
    "        else:\n",
    "            R['class_ratio'] = np.nan\n",
    "\n",
    "    C = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=classes)\n",
    "    for (i1, c1), (i2, c2) in product(enumerate(classes), enumerate(classes)):\n",
    "        R[f'true_{c1}_pred_{c2}'] = C[i1, i2]\n",
    "\n",
    "    # Threshold Measure\n",
    "    R['acc'] = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['bac'] = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['gmean'] = ms.gmean(np.diag(C) / np.sum(C, axis=1))\n",
    "    R['mcc'] = matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    if is_multiclass:\n",
    "        for avg in ('macro', 'micro'):\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                labels=classes,\n",
    "                average=avg, \n",
    "                zero_division=0\n",
    "            )\n",
    "            R[f'pre_{avg}'] = pre\n",
    "            R[f'rec_{avg}'] = rec\n",
    "            R[f'f1_{avg}'] = f1\n",
    "    else:\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true=y_true, y_pred=y_pred, pos_label=c, average='macro', zero_division=0\n",
    "        )\n",
    "        R[f'pre_macro'] = pre\n",
    "        R[f'rec_macro'] = rec\n",
    "        R[f'f1_macro'] = f1\n",
    "        \n",
    "        for c in classes:\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true, y_pred=y_pred, pos_label=c, average='binary', zero_division=0\n",
    "            )\n",
    "            R[f'pre_{c}'] = pre\n",
    "            R[f'rec_{c}'] = rec\n",
    "            R[f'f1_{c}'] = f1\n",
    "\n",
    "    # Ranking Measure\n",
    "    if is_multiclass:\n",
    "        for avg, mc in product(('macro', 'micro'), ('ovr', 'ovo')):\n",
    "            R[f'roauc_{avg}_{mc}'] = roc_auc_score(\n",
    "                y_true=y_true, y_score=y_proba,\n",
    "                average=avg, multi_class=mc, labels=classes\n",
    "            ) if not is_same_y else np.nan\n",
    "    else:\n",
    "        R[f'roauc'] = roc_auc_score(\n",
    "            y_true=y_true, y_score=y_proba[:, 1], average=None\n",
    "        ) if not is_same_y else np.nan\n",
    "        for i, c in enumerate(classes):\n",
    "            R[f'prauc_{c}'] = average_precision_score(\n",
    "                y_true=y_true, y_score=y_proba[:, i], pos_label=c, average=None\n",
    "            ) \n",
    "            R[f'prauc_ref_{c}'] = np.sum(y_true == c) / len(y_true)\n",
    "\n",
    "    # Probability Measure\n",
    "    R['log_loss'] = log_loss(y_true=y_true, y_pred=y_proba, labels=classes, normalize=True)\n",
    "\n",
    "    if not is_multiclass:\n",
    "        R[f'brier_loss'] = brier_score_loss(\n",
    "            y_true=y_true, y_proba=y_proba[:, 1], pos_label=classes[1]\n",
    "        )\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RESULTS_EVAL = []\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "for l in ['attention']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        model, pid = f[:f.index('.pkl')].split('#')\n",
    "        res = load(os.path.join(dir_l, f))\n",
    "        X, y = res.X_test, res.y_test\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_proba = res.estimator.predict_proba(X)\n",
    "        ev_test = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_proba=y_proba,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        X, y = res.X_train, res.y_train\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_proba = res.estimator.predict_proba(X)\n",
    "        ev_train = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_proba=y_proba,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        RESULTS_EVAL.append({\n",
    "            'label': l,\n",
    "            'alg': model,\n",
    "            'split': pid,\n",
    "            'n_feature': len(X.columns),\n",
    "            **{\n",
    "                f'test_{k}': v for k, v in ev_test.items()\n",
    "            },\n",
    "            **{\n",
    "                f'train_{k}': v for k, v in ev_train.items()\n",
    "            }\n",
    "        })\n",
    "    \n",
    "RESULTS_EVAL = pd.DataFrame(RESULTS_EVAL)\n",
    "RESULTS_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "SUMMARY_EVAL = []\n",
    "\n",
    "for row in RESULTS_EVAL.groupby(\n",
    "    ['label', 'alg']\n",
    ").agg(summary).reset_index().itertuples():\n",
    "    for k, v in row._asdict().items():\n",
    "        if type(v) is dict:\n",
    "            r = dict(\n",
    "                label=row.label,\n",
    "                alg=row.alg,\n",
    "                metric=k,\n",
    "                **v\n",
    "            )\n",
    "            SUMMARY_EVAL.append(r)\n",
    "\n",
    "SUMMARY_EVAL = pd.DataFrame(SUMMARY_EVAL)    \n",
    "SUMMARY_EVAL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows metrics of our interest only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_SUMMARY_EVAL = SUMMARY_EVAL.loc[\n",
    "    lambda x: x['metric'].isin(\n",
    "        ['n_feature', 'train_class_ratio', 'train_inst_0', 'train_inst_1', 'test_inst_0', 'test_inst_1', 'test_acc', 'test_f1_0' ,'test_f1_1', 'test_f1_macro', 'train_f1_0' ,'train_f1_1', 'train_f1_macro',]\n",
    "    )\n",
    "].round(3).assign(\n",
    "    mean_sd=lambda x: x['mean'].astype(str).str.cat(' (' + x['SD'].astype(str) + ')', sep='')\n",
    ").pivot(\n",
    "    index=['label', 'alg'], columns=['metric'], values=['mean_sd']\n",
    ")\n",
    "SUB_SUMMARY_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "def feature_importance(\n",
    "    estimator\n",
    "):\n",
    "    if not hasattr(estimator, 'feature_names_in_') or not hasattr(estimator, 'feature_importances_'):\n",
    "        return None\n",
    "    \n",
    "    names = estimator.feature_names_in_\n",
    "    importances = estimator.feature_importances_\n",
    "    \n",
    "    return names, importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "IMPORTANCE_EVAL = defaultdict(list)\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "for l in ['attention']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        res = load(os.path.join(dir_l, f))\n",
    "\n",
    "        f_norm = f[:f.index('.pkl')]\n",
    "        alg = f_norm[:f.rindex('#')]\n",
    "        \n",
    "        feat_imp = feature_importance(res.estimator)\n",
    "        if not feat_imp:\n",
    "            continue\n",
    "            \n",
    "        names, importance = feat_imp\n",
    "        new_names = []\n",
    "        for n in names:\n",
    "            for c in res.categories:\n",
    "                n = n.replace(f'{c}_', f'{c}=')\n",
    "            new_names.append(n)\n",
    "        \n",
    "        d = pd.DataFrame(\n",
    "            importance.reshape(1, -1),\n",
    "            columns=new_names\n",
    "        )\n",
    "        IMPORTANCE_EVAL[(l, alg)].append(d)\n",
    "        \n",
    "\n",
    "IMPORTANCE_SUMMARY = []\n",
    "\n",
    "for (l, alg), v in IMPORTANCE_EVAL.items():\n",
    "    new_v = pd.concat(\n",
    "        v, axis=0\n",
    "    ).fillna(0.0).mean().reset_index().set_axis(\n",
    "        ['feature', 'importance'], axis=1\n",
    "    ).assign(\n",
    "        label=l,\n",
    "        alg=alg\n",
    "    )\n",
    "    IMPORTANCE_SUMMARY.append(new_v)\n",
    "    \n",
    "IMPORTANCE_SUMMARY = pd.concat(IMPORTANCE_SUMMARY, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i IMPORTANCE_SUMMARY -w 26 -h 16 -u cm\n",
    "\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(stringr)\n",
    "library(patchwork)\n",
    "\n",
    "data <- IMPORTANCE_SUMMARY %>% filter(label == 'attention')\n",
    "\n",
    "p_label <- ggplot() + geom_text(\n",
    "    aes(x = .5, y = .5),\n",
    "    label = 'Attention',\n",
    "    family = 'ssp',\n",
    "    fontface = 'bold',\n",
    "    size = 4\n",
    ") + theme_void()\n",
    "\n",
    "p_rf <- ggplot(\n",
    "    data %>% filter(alg == 'rf_os') %>% top_n(n = 10, wt = importance),\n",
    "    aes(x = reorder(feature, -importance), y = importance)\n",
    ") + geom_col() +\n",
    "    THEME_DEFAULT + theme(\n",
    "        axis.text.x = element_text(angle = 90, size = 10, hjust = 1, vjust = .5),\n",
    "        axis.title.x = element_blank(),\n",
    "        axis.title.y = element_blank()\n",
    "    ) + labs(subtitle = 'Random Forest')\n",
    "\n",
    "p_xgb <- ggplot(\n",
    "    data %>% filter(alg == 'xgb_os') %>% top_n(n = 10, wt = importance),\n",
    "    aes(x = reorder(feature, -importance), y = importance)\n",
    ") + geom_col() +\n",
    "    THEME_DEFAULT + theme(\n",
    "        axis.text.x = element_text(angle = 90, size = 10, hjust = 1, vjust = .5),\n",
    "        axis.title.x = element_blank(),\n",
    "        axis.title.y = element_blank()\n",
    "    ) + labs(subtitle = 'XGBoost')\n",
    "\n",
    "p <- p_label / (p_rf | p_xgb) + plot_layout(heights = c(1.1, 10))\n",
    "\n",
    "ggsave('./fig/imp_attention.pdf', plot = p, width = 26, height = 16, unit = 'cm', device = cairo_pdf)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ray-env)",
   "language": "python",
   "name": "ray-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
