{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import os\n",
    "\n",
    "\n",
    "DEFAULT_TZ = pytz.FixedOffset(540)  # GMT+09:00; Asia/Seoul\n",
    "\n",
    "PATH_DATA = './data'\n",
    "PATH_ESM = os.path.join(PATH_DATA, 'EsmResponse.csv')\n",
    "PATH_PARTICIPANT = os.path.join(PATH_DATA, 'UserInfo.csv')\n",
    "PATH_SENSOR = os.path.join(PATH_DATA, 'Sensor')\n",
    "\n",
    "PATH_INTERMEDIATE = './intermediate'\n",
    "\n",
    "DATA_TYPES = {\n",
    "#     'EDA': 'EDA',\n",
    "    'HR': 'HRT',\n",
    "#     'RRI': 'RRI',\n",
    "#     'SkinTemperature': 'SKT',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import cloudpickle\n",
    "import ray\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "def load(path: str):\n",
    "    with open(path, mode='rb') as f:\n",
    "        return cloudpickle.load(f)\n",
    "    \n",
    "def dump(obj, path: str):\n",
    "    with open(path, mode='wb') as f:\n",
    "        cloudpickle.dump(obj, f)\n",
    "    \n",
    "def log(msg: any):\n",
    "    print('[{}] {}'.format(datetime.now().strftime('%y-%m-%d %H:%M:%S'), msg))\n",
    "\n",
    "def summary(x):\n",
    "    x = np.asarray(x)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        n = len(x)\n",
    "        # Here, uppercase np.dtype.kind corresponds to non-numeric data.\n",
    "        # Also, we view the boolean data as dichotomous categorical data.\n",
    "        if x.dtype.kind.isupper() or x.dtype.kind == 'b': \n",
    "            cnt = pd.Series(x).value_counts(dropna=False)\n",
    "            card = len(cnt)\n",
    "            cnt = cnt[:20]                \n",
    "            cnt_str = ', '.join([f'{u}:{c}' for u, c in zip(cnt.index, cnt)])\n",
    "            if card > 30:\n",
    "                cnt_str = f'{cnt_str}, ...'\n",
    "            return {\n",
    "                'n': n,\n",
    "                'cardinality': card,\n",
    "                'value_count': cnt_str\n",
    "            }\n",
    "        else: \n",
    "            x_nan = x[np.isnan(x)]\n",
    "            x_norm = x[~np.isnan(x)]\n",
    "            \n",
    "            tot = np.sum(x_norm)\n",
    "            m = np.mean(x_norm)\n",
    "            me = np.median(x_norm)\n",
    "            s = np.std(x_norm, ddof=1)\n",
    "            l, u = np.min(x_norm), np.max(x)\n",
    "            conf_l, conf_u = st.t.interval(0.95, len(x_norm) - 1, loc=m, scale=st.sem(x_norm))\n",
    "            n_nan = len(x_nan)\n",
    "            \n",
    "            return {\n",
    "                'n': n,\n",
    "                'sum': tot,\n",
    "                'mean': m,\n",
    "                'SD': s,\n",
    "                'med': me,\n",
    "                'range': (l, u),\n",
    "                'conf.': (conf_l, conf_u),\n",
    "                'nan_count': n_nan\n",
    "            }\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def on_ray(*args, **kwargs):\n",
    "    try:\n",
    "        if ray.is_initialized():\n",
    "            ray.shutdown()\n",
    "        ray.init(*args, **kwargs)\n",
    "        yield None\n",
    "    finally:\n",
    "        ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%R\n",
    "\n",
    "# library(tidyverse)\n",
    "# library(ggforce)\n",
    "# library(ggpubr)\n",
    "# library(showtext)\n",
    "# library(rmcorr)\n",
    "# library(patchwork)\n",
    "\n",
    "# # font_add_google(\n",
    "# #     name='Source Serif Pro',\n",
    "# #     family='ssp',\n",
    "# #     db_cache=FALSE\n",
    "# # )\n",
    "\n",
    "# showtext_auto()\n",
    "\n",
    "# THEME_DEFAULT <- theme_bw(\n",
    "#     base_size=10,\n",
    "#     base_family='ssp',\n",
    "# ) + theme(\n",
    "#         axis.title.x=element_text(colour='grey20', size=10, face='bold'),\n",
    "#         axis.title.y=element_text(colour='grey20', size=10, face='bold'),\n",
    "#         axis.text.x=element_text(colour='grey20', size=10),\n",
    "#         axis.text.y=element_text(colour='grey20', size=10),\n",
    "#         strip.text.x=element_text(colour='grey20', size=10, face='bold'),\n",
    "#         strip.text.y=element_text(colour='grey20', size=10, face='bold'),\n",
    "#         legend.background=element_blank(),\n",
    "#         legend.title=element_text(colour='grey20', size=10, face='bold'),\n",
    "#         legend.text=element_text(colour='grey20', size=10),\n",
    "#         legend.position='top',\n",
    "#         legend.box.spacing= unit(0, 'cm'),\n",
    "#         plot.subtitle=element_text(colour='grey20', size=10, hjust=.5),\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participationStartDate</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>PSS</th>\n",
       "      <th>PHQ</th>\n",
       "      <th>GHQ</th>\n",
       "      <th>particpationStartDateTime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P02</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>21</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P03</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P04</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P05</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>27</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      participationStartDate  age gender  openness  conscientiousness  \\\n",
       "pcode                                                                   \n",
       "P01               2019-05-08   27      M        11                 11   \n",
       "P02               2019-05-08   21      M        14                  5   \n",
       "P03               2019-05-08   24      F        10                 15   \n",
       "P04               2019-05-08   23      M        12                 11   \n",
       "P05               2019-05-08   27      F        10                 11   \n",
       "\n",
       "       neuroticism  extraversion  agreeableness  PSS  PHQ  GHQ  \\\n",
       "pcode                                                            \n",
       "P01              3             4             13   13    0    1   \n",
       "P02             12            14              5   27    6   18   \n",
       "P03              8             7             11   18    2    6   \n",
       "P04              8             6             11   20    1    9   \n",
       "P05             13            10              6   25   14    9   \n",
       "\n",
       "      particpationStartDateTime  \n",
       "pcode                            \n",
       "P01   2019-05-08 00:00:00+09:00  \n",
       "P02   2019-05-08 00:00:00+09:00  \n",
       "P03   2019-05-08 00:00:00+09:00  \n",
       "P04   2019-05-08 00:00:00+09:00  \n",
       "P05   2019-05-08 00:00:00+09:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "PARTICIPANTS = pd.read_csv(PATH_PARTICIPANT).set_index('pcode').assign(\n",
    "    particpationStartDateTime=lambda x: pd.to_datetime(\n",
    "        x['participationStartDate'], format='%Y-%m-%d'\n",
    "    ).dt.tz_localize(DEFAULT_TZ)\n",
    ")\n",
    "PARTICIPANTS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belows are some demographics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- participationStartDate: {'n': 77, 'cardinality': 3, 'value_count': '2019-05-08:27, 2019-05-16:25, 2019-04-30:25'}\n",
      "- age: {'n': 77, 'sum': 1686, 'mean': 21.896103896103895, 'SD': 3.8613619617422406, 'med': 21.0, 'range': (17, 38), 'conf.': (21.01968223607122, 22.77252555613657), 'nan_count': 0}\n",
      "- gender: {'n': 77, 'cardinality': 2, 'value_count': 'M:53, F:24'}\n",
      "- openness: {'n': 77, 'sum': 787, 'mean': 10.220779220779221, 'SD': 2.8956563505732467, 'med': 11.0, 'range': (3, 15), 'conf.': (9.563545847995773, 10.87801259356267), 'nan_count': 0}\n",
      "- conscientiousness: {'n': 77, 'sum': 820, 'mean': 10.64935064935065, 'SD': 2.3662441579221882, 'med': 11.0, 'range': (5, 15), 'conf.': (10.112279104782713, 11.186422193918586), 'nan_count': 0}\n",
      "- neuroticism: {'n': 77, 'sum': 618, 'mean': 8.025974025974026, 'SD': 2.6900108881310953, 'med': 8.0, 'range': (3, 14), 'conf.': (7.4154164477308075, 8.636531604217245), 'nan_count': 0}\n",
      "- extraversion: {'n': 77, 'sum': 703, 'mean': 9.12987012987013, 'SD': 3.0015375417426937, 'med': 9.0, 'range': (3, 15), 'conf.': (8.448604674559745, 9.811135585180514), 'nan_count': 0}\n",
      "- agreeableness: {'n': 77, 'sum': 805, 'mean': 10.454545454545455, 'SD': 2.526415468527935, 'med': 11.0, 'range': (5, 15), 'conf.': (9.881119481845285, 11.027971427245625), 'nan_count': 0}\n",
      "- PSS: {'n': 77, 'sum': 1294, 'mean': 16.805194805194805, 'SD': 7.178254737745983, 'med': 16.0, 'range': (3, 32), 'conf.': (15.175930831569763, 18.434458778819845), 'nan_count': 0}\n",
      "- PHQ: {'n': 77, 'sum': 380, 'mean': 4.935064935064935, 'SD': 4.609308837152732, 'med': 4.0, 'range': (0, 19), 'conf.': (3.888880158116967, 5.981249712012904), 'nan_count': 0}\n",
      "- GHQ: {'n': 77, 'sum': 780, 'mean': 10.12987012987013, 'SD': 5.894579689829796, 'med': 10.0, 'range': (1, 27), 'conf.': (8.791964652957894, 11.467775606782364), 'nan_count': 0}\n",
      "- particpationStartDateTime: {'n': 77, 'cardinality': 3, 'value_count': '2019-05-08 00:00:00+09:00:27, 2019-05-16 00:00:00+09:00:25, 2019-04-30 00:00:00+09:00:25'}\n"
     ]
    }
   ],
   "source": [
    "for c in PARTICIPANTS.columns:\n",
    "    print(f'- {c}:', summary(PARTICIPANTS[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels (via ESM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responseTime</th>\n",
       "      <th>scheduledTime</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>attention</th>\n",
       "      <th>stress</th>\n",
       "      <th>duration</th>\n",
       "      <th>disturbance</th>\n",
       "      <th>change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>1557278103000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>1557278986000</td>\n",
       "      <td>1.557279e+12</td>\n",
       "      <td>-3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>1557281772000</td>\n",
       "      <td>1.557282e+12</td>\n",
       "      <td>-3</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>1557287138000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>1557291117000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        responseTime  scheduledTime  valence  arousal  attention  stress  \\\n",
       "pcode                                                                      \n",
       "P01    1557278103000            NaN        0        0          0      -1   \n",
       "P01    1557278986000   1.557279e+12       -3        3          3       3   \n",
       "P01    1557281772000   1.557282e+12       -3       -2          2       2   \n",
       "P01    1557287138000            NaN        2       -1          2       0   \n",
       "P01    1557291117000            NaN        3        3          3      -3   \n",
       "\n",
       "       duration  disturbance  change  \n",
       "pcode                                 \n",
       "P01        20.0            3      -2  \n",
       "P01         5.0           -1      -3  \n",
       "P01        15.0            3      -2  \n",
       "P01        15.0            1      -1  \n",
       "P01        20.0            1       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "LABELS = pd.read_csv(PATH_ESM).set_index(\n",
    "    ['pcode']\n",
    ")\n",
    "LABELS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- responseTime: {'n': 5582, 'sum': 8694314195328000, 'mean': 1557562557385.8833, 'SD': 590915040.4254278, 'med': 1557562969500.0, 'range': (1556582982000, 1558545246000), 'conf.': (1557547052362.8618, 1557578062408.9048), 'nan_count': 0}\n",
      "- scheduledTime: {'n': 5582, 'sum': 5175814282500000.0, 'mean': 1557572760306.9517, 'SD': 591697484.8543198, 'med': 1557565860000.0, 'range': (1556586120000.0, nan), 'conf.': (1557552635074.4736, 1557592885539.4297), 'nan_count': 2259}\n",
      "- valence: {'n': 5582, 'sum': 3665, 'mean': 0.6565747044070226, 'SD': 1.4184297545899174, 'med': 1.0, 'range': (-3, 3), 'conf.': (0.6193565182132938, 0.6937928906007513), 'nan_count': 0}\n",
      "- arousal: {'n': 5582, 'sum': -529, 'mean': -0.09476890003582945, 'SD': 1.6675313128774563, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.13852326339835566, -0.051014536673303246), 'nan_count': 0}\n",
      "- attention: {'n': 5582, 'sum': 2236, 'mean': 0.4005732712289502, 'SD': 1.6113242733571864, 'med': 1.0, 'range': (-3, 3), 'conf.': (0.3582937246879792, 0.4428528177699212), 'nan_count': 0}\n",
      "- stress: {'n': 5582, 'sum': -1450, 'mean': -0.25976352561805804, 'SD': 1.6154902647587075, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.30215238363050767, -0.21737466760560845), 'nan_count': 0}\n",
      "- duration: {'n': 5582, 'sum': 141955.0, 'mean': 26.390593047034766, 'SD': 18.060980770860386, 'med': 20.0, 'range': (5.0, nan), 'conf.': (25.90782735251826, 26.87335874155127), 'nan_count': 203}\n",
      "- disturbance: {'n': 5582, 'sum': -243, 'mean': -0.04353278394840559, 'SD': 1.7587124884936127, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.0896796506833856, 0.0026140827865744204), 'nan_count': 0}\n",
      "- change: {'n': 5582, 'sum': -52, 'mean': -0.009315657470440702, 'SD': 0.9046571244275675, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.03305296077324875, 0.014421645832367342), 'nan_count': 0}\n"
     ]
    }
   ],
   "source": [
    "for c in LABELS.columns:\n",
    "    print(f'- {c}:', summary(LABELS[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belows are some demographics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- # Inst.: {'n': 77, 'sum': 5582, 'mean': 72.49350649350649, 'SD': 16.02270048911147, 'med': 74.0, 'range': (20, 110), 'conf.': (68.85679957506535, 76.13021341194762), 'nan_count': 0}\n",
      "- # Inst. - Scheduled: {'n': 76, 'sum': 3323, 'mean': 43.723684210526315, 'SD': 19.36291898394835, 'med': 43.5, 'range': (3, 83), 'conf.': (39.29906768289902, 48.14830073815361), 'nan_count': 0}\n",
      "- # Inst. - Voluntary: {'n': 77, 'sum': 2259, 'mean': 29.337662337662337, 'SD': 16.297893300742235, 'med': 27.0, 'range': (2, 74), 'conf.': (25.6384943127028, 33.03683036262187), 'nan_count': 0}\n",
      "- Samp. period: {'n': 5505, 'sum': 42240670.0, 'mean': 7673.146230699364, 'SD': 13193.471538029606, 'med': 3090.0, 'range': (1.0, 136446.0), 'conf.': (7324.548923384188, 8021.743538014541), 'nan_count': 0}\n",
      "- responseTime: {'n': 5582, 'sum': 8694314195328000, 'mean': 1557562557385.8833, 'SD': 590915040.4254278, 'med': 1557562969500.0, 'range': (1556582982000, 1558545246000), 'conf.': (1557547052362.8618, 1557578062408.9048), 'nan_count': 0}\n",
      "- scheduledTime: {'n': 5582, 'sum': 5175814282500000.0, 'mean': 1557572760306.9517, 'SD': 591697484.8543198, 'med': 1557565860000.0, 'range': (1556586120000.0, nan), 'conf.': (1557552635074.4736, 1557592885539.4297), 'nan_count': 2259}\n",
      "- valence: {'n': 5582, 'sum': 3665, 'mean': 0.6565747044070226, 'SD': 1.4184297545899174, 'med': 1.0, 'range': (-3, 3), 'conf.': (0.6193565182132938, 0.6937928906007513), 'nan_count': 0}\n",
      "- arousal: {'n': 5582, 'sum': -529, 'mean': -0.09476890003582945, 'SD': 1.6675313128774563, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.13852326339835566, -0.051014536673303246), 'nan_count': 0}\n",
      "- attention: {'n': 5582, 'sum': 2236, 'mean': 0.4005732712289502, 'SD': 1.6113242733571864, 'med': 1.0, 'range': (-3, 3), 'conf.': (0.3582937246879792, 0.4428528177699212), 'nan_count': 0}\n",
      "- stress: {'n': 5582, 'sum': -1450, 'mean': -0.25976352561805804, 'SD': 1.6154902647587075, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.30215238363050767, -0.21737466760560845), 'nan_count': 0}\n",
      "- duration: {'n': 5582, 'sum': 141955.0, 'mean': 26.390593047034766, 'SD': 18.060980770860386, 'med': 20.0, 'range': (5.0, nan), 'conf.': (25.90782735251826, 26.87335874155127), 'nan_count': 203}\n",
      "- disturbance: {'n': 5582, 'sum': -243, 'mean': -0.04353278394840559, 'SD': 1.7587124884936127, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.0896796506833856, 0.0026140827865744204), 'nan_count': 0}\n",
      "- change: {'n': 5582, 'sum': -52, 'mean': -0.009315657470440702, 'SD': 0.9046571244275675, 'med': 0.0, 'range': (-3, 3), 'conf.': (-0.03305296077324875, 0.014421645832367342), 'nan_count': 0}\n"
     ]
    }
   ],
   "source": [
    "inst = LABELS.groupby('pcode').count().iloc[:, -1]\n",
    "inst_sch = LABELS.loc[lambda x: ~x['scheduledTime'].isna(), :].groupby('pcode').count().iloc[:, -1]\n",
    "inst_vol = LABELS.loc[lambda x: x['scheduledTime'].isna(), :].groupby('pcode').count().iloc[:, -1]\n",
    "resp_time = LABELS.assign(\n",
    "    timestamp=lambda x: pd.to_datetime(x['responseTime'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ)\n",
    ")\n",
    "sam = np.concatenate([\n",
    "    (resp_time.loc[p, 'timestamp'].array - resp_time.loc[p, 'timestamp'].array.shift(1)).dropna().total_seconds()\n",
    "    for p in LABELS.index.unique()\n",
    "])\n",
    "\n",
    "print('- # Inst.:', summary(inst))\n",
    "print('- # Inst. - Scheduled:', summary(inst_sch))\n",
    "print('- # Inst. - Voluntary:', summary(inst_vol))\n",
    "print('- Samp. period:', summary(sam))\n",
    "for c in LABELS.columns:\n",
    "    print(f'- {c}:', summary(LABELS[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LABELS.loc[\n",
    "    :, lambda x: ~x.columns.isin(['responseTime', 'scheduledTime'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%R -i data -w 16 -h 6 -u cm\n",
    "\n",
    "# data <- data %>% pivot_longer(\n",
    "#     cols = c('valence', 'arousal', 'attention', 'stress', 'duration', 'disturbance', 'change'),\n",
    "#     names_to = 'metric'\n",
    "# )\n",
    "\n",
    "# p_rest <- ggplot(\n",
    "#     data %>% filter(metric != 'duration'), aes(x=metric, y=value)\n",
    "# ) + geom_boxplot(\n",
    "# ) + geom_point(\n",
    "#     data = data %>% filter(\n",
    "#         metric != 'duration'\n",
    "#     ) %>% group_by(\n",
    "#         metric\n",
    "#     ) %>% summarise(\n",
    "#         mean = mean(value, na.rm=TRUE)\n",
    "#     ),\n",
    "#     mapping=aes(x=metric, y=mean),\n",
    "#     shape=21,\n",
    "#     stroke=1,\n",
    "#     size=2,\n",
    "#     fill='white'\n",
    "# ) + scale_x_discrete(\n",
    "#     name=NULL,\n",
    "#     limits=c('valence', 'arousal', 'stress', 'attention', 'disturbance', 'change'),\n",
    "#     labels=c('Valence', 'Arousal', 'Stress', 'Attent.', 'Disturb.', 'Change'),\n",
    "# ) + scale_y_continuous(\n",
    "#     name='Response',\n",
    "#     breaks=-3:3\n",
    "# ) + THEME_DEFAULT\n",
    "\n",
    "# p_duration <- ggplot(\n",
    "#     data %>% filter(metric == 'duration'), aes(x=metric, y=value)\n",
    "# ) + geom_boxplot(\n",
    "# ) + geom_point(\n",
    "#     data = data %>% filter(\n",
    "#         metric == 'duration'\n",
    "#     ) %>% group_by(\n",
    "#         metric\n",
    "#     ) %>% summarise(\n",
    "#         mean = mean(value, na.rm=TRUE)\n",
    "#     ),\n",
    "#     mapping=aes(x=metric, y=mean),\n",
    "#     shape=21,\n",
    "#     stroke=1,\n",
    "#     size=2,\n",
    "#     fill='white'\n",
    "# )+ scale_x_discrete(\n",
    "#     name=NULL,\n",
    "#     limits=c('duration'),\n",
    "#     labels=c('Duration'),\n",
    "# ) + scale_y_continuous(\n",
    "#     name=NULL,\n",
    "#     breaks=seq(from=5, to=60, by=10)\n",
    "# ) + THEME_DEFAULT\n",
    "\n",
    "# p <- p_rest + p_duration + plot_layout(widths=c(4, 0.8))\n",
    "# ggsave('./fig/dist-labels.pdf', plot=p, width=16, height=6, unit='cm', device=cairo_pdf)\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each participant reported their labels multiple times (i.e., repeated measure), repeated measure correlation between affect labels were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LABELS.reset_index()[[\n",
    "    'pcode', 'valence', 'arousal', 'stress', 'attention', 'disturbance', 'change'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%R -i data \n",
    "\n",
    "# com <- combn(c('valence', 'arousal', 'stress', 'attention', 'disturbance', 'change'), 2)\n",
    "\n",
    "# for(i in 1:ncol(com)) {\n",
    "#     a <- com[, i][1]\n",
    "#     b <- com[, i][2]\n",
    "#     r <- rmcorr(participant = 'pcode', measure1=a, measure2=b, dataset=data)\n",
    "#     cat(a, '-', b, ': R =', r$r, '(p =', r$p, ') \\n')\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def _load_data(\n",
    "    name: str\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    paths = [\n",
    "        (d, os.path.join(PATH_SENSOR, d, f'{name}.csv'))\n",
    "        for d in os.listdir(PATH_SENSOR)\n",
    "        if d.startswith('P')\n",
    "    ]\n",
    "    return pd.concat(\n",
    "        filter(\n",
    "            lambda x: len(x.index), \n",
    "            [\n",
    "                pd.read_csv(p).assign(pcode=pcode)\n",
    "                for pcode, p in paths\n",
    "                if os.path.exists(p)\n",
    "            ]\n",
    "        ), ignore_index=True\n",
    "    ).assign(\n",
    "        timestamp=lambda x: pd.to_datetime(x['timestamp'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ)\n",
    "    ).set_index(\n",
    "        ['pcode', 'timestamp']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from datetime import timedelta as td\n",
    "\n",
    "\n",
    "# STATS = []\n",
    "\n",
    "# for data_type in DATA_TYPES:\n",
    "#     dat = _load_data(data_type)\n",
    "#     inst = dat.groupby('pcode').count().iloc[:, -1]\n",
    "#     samp = np.concatenate([\n",
    "#         (dat.loc[(p,), :].index.array - dat.loc[(p,), :].index.array.shift(1)).dropna().total_seconds()\n",
    "#         for p in dat.index.get_level_values('pcode').unique()\n",
    "#     ])\n",
    "#     inst, samp = summary(inst), summary(samp)\n",
    "    \n",
    "#     print('#'*5, data_type, '#'*5)\n",
    "#     print('- # Inst.:', inst)\n",
    "#     print('- Samp. period:', samp)\n",
    "    \n",
    "#     for c in dat.columns:\n",
    "#         print(f'- {c}:', summary(dat[c]))\n",
    "        \n",
    "#     del dat\n",
    "#     gc.collect()\n",
    "    \n",
    "# STATS = pd.DataFrame(STATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label\n",
    "\n",
    "Because we intended to collected participants' responses to ESMs not voluntary responses, we screend out some responses as follows:\n",
    "* We first screen out ESM responses that does not have 'scheduledTime' (meaning that a given ESM was expired or participants voluntarily reported their affective states regardless of ESM delivery). \n",
    "* Since we will evaluate our model using LOSO, the small number of responses for each participant might lead to inappropriate performance evaluation. We emprically set the number of the minimum responses upon ESM delivery as 5 per day (i.e., a half of our guides), so that we excluded participants whose responses to ESM less than 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from participants with enough responses: 2619\n",
      "{'n': 47, 'sum': 2619, 'mean': 55.723404255319146, 'SD': 13.076201628480542, 'med': 52.0, 'range': (36, 83), 'conf.': (51.88408762653044, 59.562720884107854), 'nan_count': 0}\n",
      "# Participants whose responses to ESM delivery were less then 35\n",
      "# participants = 29 / #response = 704\n"
     ]
    }
   ],
   "source": [
    "LABELS_VALID = LABELS.loc[\n",
    "    lambda x: ~x['scheduledTime'].isna(), :\n",
    "]\n",
    "# print(LABELS_VALID)\n",
    "# print(f'# Non-voluntary response: {len(LABELS_VALID)}')\n",
    "# print(summary(LABELS_VALID.groupby('pcode').count().iloc[:, -1]))\n",
    "\n",
    "excl_pcode = LABELS_VALID.loc[\n",
    "    lambda x: ~x['scheduledTime'].isna()\n",
    "].groupby('pcode').count().iloc[:, -1].loc[lambda y: y < 35]\n",
    "\n",
    "LABELS_VALID = LABELS_VALID.loc[\n",
    "    lambda x:  ~x.index.get_level_values('pcode').isin(excl_pcode.index), :\n",
    "]\n",
    "print(f'# Response from participants with enough responses: {len(LABELS_VALID)}')\n",
    "print(summary(LABELS_VALID.groupby('pcode').count().iloc[:, -1]))\n",
    "\n",
    "print('# Participants whose responses to ESM delivery were less then 35')\n",
    "# print(excl_pcode, f'#participants = {len(excl_pcode)} / #response = {sum(excl_pcode)}')\n",
    "print(f'# participants = {len(excl_pcode)} / #response = {sum(excl_pcode)}')\n",
    "\n",
    "# LABELS_VALID # 참여자: 47명, 응답 2619개\n",
    "# 35개의 응답 보다 작은 참여자: 29명 (47/2619 -> 29/704)\n",
    "\n",
    "# 연구는 47명으로 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider binary classifications for valence, arousal, stress, and disturbance, in which a label value greater than 0 is \"HIGH\" (1) and the rest is \"LOW\" (0), at the arrival of ESM prompts (*scheduledTime*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>attention</th>\n",
       "      <th>attention_bin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcode</th>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">P01</th>\n",
       "      <th>2019-05-08 10:26:00+09:00</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08 11:13:00+09:00</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08 15:56:00+09:00</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08 16:41:00+09:00</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08 17:23:00+09:00</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">P80</th>\n",
       "      <th>2019-05-05 21:57:00+09:00</th>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06 15:06:00+09:00</th>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06 15:53:00+09:00</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06 19:39:00+09:00</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06 21:08:00+09:00</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2619 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 attention  attention_bin\n",
       "pcode timestamp                                          \n",
       "P01   2019-05-08 10:26:00+09:00          3              1\n",
       "      2019-05-08 11:13:00+09:00          2              1\n",
       "      2019-05-08 15:56:00+09:00          3              1\n",
       "      2019-05-08 16:41:00+09:00          3              1\n",
       "      2019-05-08 17:23:00+09:00          3              1\n",
       "...                                    ...            ...\n",
       "P80   2019-05-05 21:57:00+09:00         -3              0\n",
       "      2019-05-06 15:06:00+09:00         -2              0\n",
       "      2019-05-06 15:53:00+09:00          3              1\n",
       "      2019-05-06 19:39:00+09:00         -1              0\n",
       "      2019-05-06 21:08:00+09:00          3              1\n",
       "\n",
       "[2619 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "LABELS_PROC = (\n",
    "    LABELS_VALID\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        timestamp=lambda x: pd.to_datetime(x['scheduledTime'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ),\n",
    "        attention_bin=lambda x: np.where(x['attention'] > 0, 1, 0)\n",
    "    )\n",
    "    .loc[:, ['pcode', 'timestamp', 'attention', 'attention_bin']]  # attention: 연속형, attention_bin: 이진분류된 데이터\n",
    "    .set_index(['pcode', 'timestamp'])\n",
    ")\n",
    "\n",
    "LABELS_PROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 attention_bin\n",
      "pcode timestamp                               \n",
      "P01   2019-05-08 10:16:00+09:00              1\n",
      "      2019-05-08 10:17:00+09:00              1\n",
      "      2019-05-08 10:18:00+09:00              1\n",
      "      2019-05-08 10:19:00+09:00              1\n",
      "      2019-05-08 10:20:00+09:00              1\n",
      "      2019-05-08 10:21:00+09:00              1\n",
      "      2019-05-08 10:22:00+09:00              1\n",
      "      2019-05-08 10:23:00+09:00              1\n",
      "      2019-05-08 10:24:00+09:00              1\n",
      "      2019-05-08 10:25:00+09:00              1\n",
      "- attention_bin: {'n': 54999, 'cardinality': 2, 'value_count': '0:27552, 1:27447'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/2547955742.py:19: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# 10분 범위 설정\n",
    "TIME_DELTA = timedelta(minutes=10)\n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "rows = []\n",
    "\n",
    "# 각 pcode별로 반복\n",
    "for pcode, group in LABELS_PROC.groupby(level='pcode'):\n",
    "    group = group.reset_index()\n",
    "\n",
    "    # 각 attention_bin이 있는 행에 대해\n",
    "    for _, row in group.iterrows():\n",
    "        ts = row['timestamp']\n",
    "        bin_val = row['attention_bin']\n",
    "\n",
    "        # ±10분 범위 내 timestamp 생성\n",
    "        timestamps = pd.date_range(ts - TIME_DELTA, ts + TIME_DELTA, freq='T', tz=DEFAULT_TZ)\n",
    "\n",
    "        for t in timestamps:\n",
    "            rows.append((pcode, t, bin_val))\n",
    "\n",
    "# 확장된 데이터프레임 생성\n",
    "LABELS_PROC = pd.DataFrame(rows, columns=['pcode', 'timestamp', 'attention_bin'])\n",
    "\n",
    "# 중복 제거 및 인덱스 정렬\n",
    "LABELS_PROC = (\n",
    "    LABELS_PROC\n",
    "    .drop_duplicates(subset=['pcode', 'timestamp'], keep='last')\n",
    "    .set_index(['pcode', 'timestamp'])\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# 확인 출력\n",
    "print(LABELS_PROC.head(10))\n",
    "\n",
    "# 이후 코드에 변수명 통일\n",
    "inst = LABELS_PROC.groupby('pcode').count().iloc[:, -1]\n",
    "\n",
    "for c in [c for c in LABELS_PROC.columns if c.endswith('_bin')]:\n",
    "    print(f'- {c}:', summary(LABELS_PROC[c].astype(object)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- attention_bin: {'n': 54999, 'cardinality': 2, 'value_count': '0:27552, 1:27447'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "inst = LABELS_PROC.groupby('pcode').count().iloc[:, -1]\n",
    "for c in [c for c in LABELS_PROC.columns if c.endswith('_bin')]:\n",
    "    print(f'- {c}:', summary(LABELS_PROC[c].astype(object)))\n",
    "\n",
    "# 총 응답으로 볼 때, attention의 0, 1 클래스는 balanced\n",
    "# 데이터셋으르 합칠 경우, oversampling 필요 없을 듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 평균 및 표준편차:\n",
      "  - ratio_0: 평균 = 0.500, 표준편차 = 0.206\n",
      "  - ratio_1: 평균 = 0.500, 표준편차 = 0.206\n",
      "  - imbalance_ratio: 평균 = 0.665, 표준편차 = 0.121\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inst = LABELS_PROC.groupby('pcode').count().iloc[:, -1]\n",
    "\n",
    "for c in [col for col in LABELS_PROC.columns if col.endswith('_bin')]:\n",
    "    counts = LABELS_PROC.groupby('pcode')[c].value_counts().unstack(fill_value=0)\n",
    "    ratios = counts.div(counts.sum(axis=1), axis=0)\n",
    "    stats = pd.DataFrame({\n",
    "        'n_total': counts.sum(axis=1),\n",
    "        'n_class_0': counts.get(0, 0),\n",
    "        'n_class_1': counts.get(1, 0),\n",
    "        'ratio_0': ratios.get(0, 0),\n",
    "        'ratio_1': ratios.get(1, 0),\n",
    "        'imbalance_ratio': counts.max(axis=1) / counts.sum(axis=1)\n",
    "    })\n",
    "    print(\"클래스별 평균 및 표준편차:\")\n",
    "    for col in ['ratio_0', 'ratio_1', 'imbalance_ratio']:\n",
    "        mean = stats[col].mean()\n",
    "        std = stats[col].std()\n",
    "        print(f\"  - {col}: 평균 = {mean:.3f}, 표준편차 = {std:.3f}\")\n",
    "\n",
    "# imbalance_ratio: 각 참가자의 샘플의 다수 클래스 비율\n",
    "# 값이 0.5에 가까우면 균형된 데이터\n",
    "# 문제: 전체 데이터에서는 균형, 개별 참가자 단위로는 불균형 -> LOGO시 문제 발생 가능성.\n",
    "# 해결: class_weight='balanced', oversampling, 불균형 참가자 제거, Leave-Multiple-Groups-Out\n",
    "# 평가지표? F1, ROC-AUC가 불균형에 민감한지?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each type of sensor data, we applied different preprocessing. Detailed decsription is provided in the paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Union\n",
    "\n",
    "# IQR -> 이상치 제거\n",
    "def _remove_outliers_iqr(series: pd.Series) -> pd.Series:\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return series.clip(lower=lower, upper=upper)\n",
    "\n",
    "# 로그 정규화 + Z-score\n",
    "def _log_normalize(series: pd.Series) -> pd.Series:\n",
    "    series = series.clip(lower=1)\n",
    "    log_vals = np.log1p(series)\n",
    "    return (log_vals - log_vals.mean()) / log_vals.std()\n",
    "\n",
    "# SkinTemperature.csv\n",
    "def _proc_skin_temperature(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    temp = data['temperature'].astype('float32')\n",
    "    temp = _remove_outliers_iqr(temp)\n",
    "    return (temp - temp.mean()) / temp.std()\n",
    "\n",
    "# RRI.csv\n",
    "def _proc_rri(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    rri = data['interval'].astype('float32')\n",
    "    rri = _remove_outliers_iqr(rri)\n",
    "    return (rri - rri.mean()) / rri.std()\n",
    "\n",
    "# HR.csv\n",
    "def _proc_hr(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    hr = data['bpm'].astype('float32')\n",
    "    hr = _remove_outliers_iqr(hr)\n",
    "    return (hr - hr.mean()) / hr.std()\n",
    "\n",
    "# EDA.csv\n",
    "def _proc_eda(data: pd.DataFrame) -> Union[pd.Series, Dict[str, pd.Series]]:\n",
    "    eda = data['resistance'].astype('float32')\n",
    "    eda = _remove_outliers_iqr(eda)\n",
    "    return _log_normalize(eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:20:16,729\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_process pid=83923)\u001b[0m [25-06-03 23:20:17] Begin to processing data: HR\n",
      "\u001b[2m\u001b[36m(_process pid=83923)\u001b[0m [25-06-03 23:20:25] Complete processing data: HR\n",
      "[25-06-03 23:20:26] [SAVE] done in 0.1s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from functools import reduce\n",
    "import time\n",
    "\n",
    "FUNC_PROC = {\n",
    "#     'EDA': _proc_eda,\n",
    "    'HR': _proc_hr,\n",
    "#     'RRI': _proc_rri,\n",
    "#     'SkinTemperature': _proc_skin_temperature,\n",
    "}\n",
    "\n",
    "\n",
    "def _process(data_type: str):\n",
    "    log(f'Begin to processing data: {data_type}')\n",
    "    \n",
    "    abbrev = DATA_TYPES[data_type]\n",
    "    data_raw = _load_data(data_type)\n",
    "    data_proc = FUNC_PROC[data_type](data_raw)\n",
    "    result = dict()\n",
    "    \n",
    "    if type(data_proc) is dict:\n",
    "        for k, v in data_proc.items():\n",
    "            result[f'{abbrev}_{k}'] = v\n",
    "    else:\n",
    "        result[abbrev] = data_proc\n",
    "        \n",
    "    log(f'Complete processing data: {data_type}')\n",
    "    return result\n",
    "\n",
    "\n",
    "with on_ray(num_cpus=12):\n",
    "    jobs = []\n",
    "    \n",
    "    func = ray.remote(_process).remote\n",
    "    \n",
    "    for data_type in DATA_TYPES:\n",
    "        job = func(data_type)\n",
    "        jobs.append(job)\n",
    "\n",
    "    jobs = ray.get(jobs)\n",
    "    \n",
    "    # 메모리 최적화를 위해 추가 \n",
    "    combined_result = {}\n",
    "    for d in jobs:\n",
    "        combined_result |= d\n",
    "    \n",
    "    t0 = time.time()\n",
    "    dump(combined_result, os.path.join(PATH_INTERMEDIATE, 'proc.pkl'))\n",
    "    log(f'[SAVE] done in {time.time() - t0:.1f}s')\n",
    "    \n",
    "    del jobs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/1220431816.py:36: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  (v.loc[(p,)].index.array - v.loc[(p,)].index.array.shift(1)).dropna().total_seconds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### HRT #####\n",
      "- Samp. period: {'n': 12126885, 'sum': 12080570.162999991, 'mean': 0.9961808133745799, 'SD': 0.013881222953694618, 'med': 0.996, 'range': (0.956, 1.036), 'conf.': (0.9961730006730264, 0.9961886260761333), 'nan_count': 0}\n",
      "- Values {'n': 13621023, 'sum': 114.31421, 'mean': 8.392483e-06, 'SD': 1.0, 'med': -0.034554552, 'range': (-2.3233254, 2.2542164), 'conf.': (-0.0005226671196859384, 0.0005394520863327485), 'nan_count': 0}\n",
      "\n",
      "# categorical data: 0/# numeric data: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "# DATA_ITEMS:\n",
    "# [('EDA', pcode  timestamp                       \n",
    "# P19    2019-05-08 09:00:00.029000+09:00     10070.0\n",
    "#        2019-05-08 09:00:00.233000+09:00     10010.0\n",
    "#        2019-05-08 09:00:00.434000+09:00     10160.0\n",
    "#        2019-05-08 09:00:00.640000+09:00     10130.0\n",
    "#        2019-05-08 09:00:00.842000+09:00     10130.0\n",
    "\n",
    "def _remove_outliers_iqr_np(arr: np.ndarray) -> np.ndarray: # IQR 방식으로 샘플링 주기 이상치 제거\n",
    "    q1 = np.percentile(arr, 25)\n",
    "    q3 = np.percentile(arr, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return arr[(arr >= lower) & (arr <= upper)]\n",
    "\n",
    "\n",
    "\n",
    "DATA = load(os.path.join(PATH_INTERMEDIATE, 'proc.pkl'))\n",
    "#categorial, numeric 변수 수 계산\n",
    "N_NUMERIC, N_CATEGORICAL = 0, 0\n",
    "\n",
    "for k, v in DATA.items():\n",
    "    if v.dtype.kind.isupper() or v.dtype.kind == 'b': \n",
    "        N_CATEGORICAL += 1\n",
    "    else:\n",
    "        N_NUMERIC += 1\n",
    "\n",
    "    inst = v.groupby('pcode').count()\n",
    "    \n",
    "    # 샘플링 주기 계산 및 이상치 제거\n",
    "    sam = np.concatenate([\n",
    "        (v.loc[(p,)].index.array - v.loc[(p,)].index.array.shift(1)).dropna().total_seconds()\n",
    "        for p in v.index.get_level_values('pcode').unique()\n",
    "    ])\n",
    "    sam = _remove_outliers_iqr_np(sam)\n",
    "\n",
    "    \n",
    "    print('#'*5, k, '#'*5, )\n",
    "#     print('- # Inst.:', inst)\n",
    "    print('- Samp. period:', summary(sam))\n",
    "    print('- Values', summary(v))\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "print(f'# categorical data: {N_CATEGORICAL}/# numeric data: {N_NUMERIC}')\n",
    "del DATA\n",
    "gc.collect()\n",
    "\n",
    "# IQR 방식으로 샘플링 주기 이상치 제거 전\n",
    "# | 센서   | 샘플링 주기 평균 (s)     | 샘플링 주기 SD    \n",
    "# |-------|----------------------|----------------\n",
    "# | EDA   | 0.514                | 140.55         \n",
    "# | HRT   | 3.008                | 362.92         \n",
    "# | RRI   | 1.985                | 276.20         \n",
    "# | SKT   | 77.03                | 1,719.19       \n",
    "\n",
    "# IQR 방식으로 샘플링 주기 이상치 제거 후\n",
    "# | 센서   | 샘플링 주기 평균 (s)     | 샘플링 주기 SD    \n",
    "# |-------|----------------------|----------------\n",
    "# | EDA   | 0.199                | 0.013          \n",
    "# | HRT   | 0.996                | 0.014          \n",
    "# | RRI   | 0.761                | 0.172          \n",
    "# | SKT   | 30.08                | 0.014          \n",
    "\n",
    "# - EDA:\n",
    "#   · 많이 이상함. 전처리 필요\n",
    "#   · 매우 빠른 샘플링(0.5초), 샘플링 주기의 표준편차가 매우 큼. \n",
    "#   · 샘플 누락, 중단, 오류 구간의 존재 가능성?, 극단적으로 큰 샘플링 주기가 존재?\n",
    "#     -> 이상치 제거, 리샘플링, (스케일이 너무 큼 -> 정규화(로그 변환, 정규화))\n",
    "#   · 측정값이 큼 -> log 변환 고려?\n",
    "\n",
    "# - HRT:\n",
    "#   · 평균 주기 3초, SD=362초 -> 이것도 이상함, 이상치 제거 필요?\n",
    "#   · 평균, 표준편차 왜곡?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Callable, Union, Tuple, List, Optional, Iterable\n",
    "from datetime import timedelta as td\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "import ray\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "def _safe_na_check(_v):\n",
    "    _is_nan_inf = False\n",
    "    try:\n",
    "        _is_nan_inf = np.isnan(_v) or np.isinf(_v)\n",
    "    except:\n",
    "        _is_nan_inf = False\n",
    "    return _is_nan_inf or _v is None\n",
    "\n",
    "# _extract: 한 명의 참가자(pid)의 feature 생성\n",
    "# _extract 출력:\n",
    "# X: DataFrame(n, f) / n = 라벨 개수, f = 추출된 feature 수 / row: timestamp에 대한 feature vector, col: 센서, 시간 기반 피쳐\n",
    "# y: np.ndarray(n,) / timestamp별 라벨(1D)\n",
    "# group: np.ndarray(n,) / 참가자 ID 반복 배열 (모델 그룹 분리용) / 각 row 별 pid - KFold, LOGO에 사용 / 입력 파라미터의 pid와 동일값 반복\n",
    "# date_times: np.ndarray(n,) / 각 sample의 timestamp\n",
    "def _extract(\n",
    "        pid: str,\n",
    "        data: Dict[str, pd.Series], # pcode, timestamp으로 구성된 센서의 시계열 데이터\n",
    "        label: pd.Series, # timestamp별 라벨\n",
    "        label_values: List[str], # 가능한 class: [0, 1]\n",
    "        window_data: Dict[str, Union[int, Callable[[pd.Timestamp], int]]], # 센서별 time-window 크기\n",
    "        window_label: Dict[str, Union[int, Callable[[pd.Timestamp], int]]], # label의 time-window 크기\n",
    "        categories: Dict[str, Optional[List[any]]] = None, # 범주형 센서\n",
    "        resample_s: Dict[str, float] = None # 센서별 리샘플링 간격\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    _s = time.time()\n",
    "    log(f\"Begin feature extraction on {pid}'s data.\")\n",
    "\n",
    "    categories = categories or dict()\n",
    "    resample_s = resample_s or dict()\n",
    "\n",
    "    X, y, date_times = [], [], [] # 상단 주석 참고\n",
    " \n",
    "    for timestamp in label.index:\n",
    "        row = dict() # 각 timestamp 별 feature을 저장\n",
    "\n",
    "        label_cur = label.at[timestamp]\n",
    "        t = timestamp - td(milliseconds=1)\n",
    "\n",
    "        # Features from sensor data\n",
    "        for d_key, d_val in data.items():\n",
    "            is_numeric = d_key not in categories\n",
    "            cats = categories.get(d_key) or list()\n",
    "            d_val = d_val.sort_index()\n",
    "\n",
    "            if is_numeric or cats:\n",
    "                try:\n",
    "                    v = d_val.loc[:t].iloc[-1]             # t 시점 직전 가장 마지막 센서 값을 사용\n",
    "                except (KeyError, IndexError):\n",
    "                    v = 0\n",
    "\n",
    "                if is_numeric:\n",
    "                    row[f'{d_key}#VAL'] = v\n",
    "                else:\n",
    "                    for c in cats:\n",
    "                        row[f'{d_key}#VAL={c}'] = v == c\n",
    "\n",
    "            # catogorial 데이터의 최근 상태 변화 시간\n",
    "            if not is_numeric:\n",
    "                try:\n",
    "                    v = d_val.loc[:t]\n",
    "                    row[f'{d_key}#DSC'] = (t - v.index[-1]).total_seconds() if len(v) else -1.0\n",
    "                    for c in cats:\n",
    "                        v_sub = v.loc[lambda x: x == c].index\n",
    "                        row[f'{d_key}#DSC={c}'] = (t - v_sub[-1]).total_seconds() if len(v_sub) else -1.0\n",
    "                except (KeyError, IndexError):\n",
    "                    row[f'{d_key}#DSC'] = -1.0\n",
    "                    for c in cats:\n",
    "                        row[f'{d_key}#DSC={c}'] = -1.0\n",
    "\n",
    "            # Time-window 기반 피처 (resampling 포함)\n",
    "            sample_rate = resample_s.get(d_key) or 1\n",
    "            d_val_res = d_val.resample(f'{sample_rate}s', origin='start')\n",
    "            if is_numeric:\n",
    "                \"\"\" 보간 방식 추가 부분 \"\"\"\n",
    "                if d_key == \"interval\":\n",
    "                    interval_series = d_val.dropna()\n",
    "                    if len(interval_series) >= 4:\n",
    "                        ts = interval_series.index.view(np.int64) // 10**6\n",
    "                        cs = CubicSpline(ts, interval_series.values)\n",
    "                        full_ts = d_val.index.view(np.int64) // 10**6\n",
    "                        d_val[:] = cs(full_ts)\n",
    "                else: \n",
    "                    d_val_res = d_val_res.mean().interpolate(method='linear').dropna()\n",
    "            else:\n",
    "                d_val_res = d_val_res.ffill().dropna()\n",
    "\n",
    "            for w_key, w_val in window_data.items():\n",
    "                w_val = w_val(t) if isinstance(w_val, Callable) else w_val\n",
    "                try:\n",
    "                    v = d_val_res.loc[t - td(seconds=w_val):t]\n",
    "                    # numeric 데이터일 경우에만 변환\n",
    "                    if is_numeric:\n",
    "                        v_arr = v.values.astype(np.float64)\n",
    "                    else:\n",
    "                        v_arr = v\n",
    "                except (KeyError, IndexError):\n",
    "                    continue\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore')\n",
    "\n",
    "                    if is_numeric:\n",
    "                        hist, _ = np.histogram(v, bins='doane', density=False)\n",
    "                        std = np.sqrt(np.var(v, ddof=1)) if len(v) > 1 else 0\n",
    "                        v_norm = (v - np.mean(v)) / std if std != 0 else np.zeros(len(v))\n",
    "                        # window_data 기준으로 구간 추출 - 통계 기반\n",
    "                        row[f'{d_key}#AVG#{w_key}'] = np.float32(np.mean(v_arr))\n",
    "                        row[f'{d_key}#STD#{w_key}'] = np.float32(std)\n",
    "                        row[f'{d_key}#SKW#{w_key}'] = np.float32(stats.skew(v_arr, bias=False))\n",
    "                        row[f'{d_key}#KUR#{w_key}'] = np.float32(stats.kurtosis(v_arr, bias=False))\n",
    "                        row[f'{d_key}#ASC#{w_key}'] = np.float32(np.sum(np.abs(np.diff(v_arr))))\n",
    "                        row[f'{d_key}#BEP#{w_key}'] = np.float32(stats.entropy(hist))\n",
    "                        row[f'{d_key}#MED#{w_key}'] = np.float32(np.median(v_arr))\n",
    "                        # TSC: 시계열 복잡성. √(Δ값 제곱합) \n",
    "                        row[f'{d_key}#TSC#{w_key}'] = np.float32(np.sqrt(np.sum(np.power(np.diff(v_norm), 2))))\n",
    "                    else:\n",
    "                        cnt = v.value_counts()\n",
    "                        val, sup = cnt.index, cnt.values\n",
    "                        hist = {k: v for k, v in zip(val, sup)}\n",
    "\n",
    "                        row[f'{d_key}#ETP#{w_key}'] = stats.entropy(sup / len(v))\n",
    "                        row[f'{d_key}#ASC#{w_key}'] = np.sum(v.values[1:] != v.values[:-1])\n",
    "\n",
    "                        if len(cats) == 2:\n",
    "                            c = cats[0]\n",
    "                            row[f'{d_key}#DUR#{w_key}'] = hist[c] / len(v) if c in hist else 0\n",
    "                        else:\n",
    "                            for c in cats:\n",
    "                                row[f'{d_key}#DUR={c}#{w_key}'] = hist[c] / len(v) if c in hist else 0\n",
    "\n",
    "        # 시간 기반 피처: one-hot encoding\n",
    "        day_of_week = ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN'][t.isoweekday() - 1]\n",
    "        is_weekend = 'Y' if t.isoweekday() > 5 else 'N'\n",
    "        hour = t.hour\n",
    "\n",
    "        # 시간대별 패턴: 비정기적 설문조사 주기에 맞게 하루를 7개의 시간대로 분할\n",
    "        if 6 <= hour < 9:\n",
    "            hour_name = 'DAWN'\n",
    "        elif 9 <= hour < 12:\n",
    "            hour_name = 'MORNING'\n",
    "        elif 12 <= hour < 15:\n",
    "            hour_name = 'AFTERNOON'\n",
    "        elif 15 <= hour < 18:\n",
    "            hour_name = 'LATE_AFTERNOON'\n",
    "        elif 18 <= hour < 21:\n",
    "            hour_name = 'EVENING'\n",
    "        elif 21 <= hour < 24:\n",
    "            hour_name = 'NIGHT'\n",
    "        else:\n",
    "            hour_name = 'MIDNIGHT'\n",
    "\n",
    "        for d in ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN']:\n",
    "            row[f'ESM#DOW={d}'] = d == day_of_week\n",
    "\n",
    "        for d in ['Y', 'N']:\n",
    "            row[f'ESM#WKD={d}'] = d == is_weekend\n",
    "\n",
    "        for d in ['DAWN', 'MORNING', 'AFTERNOON', 'LATE_AFTERNOON', 'EVENING', 'NIGHT', 'MIDNIGHT']:\n",
    "            row[f'ESM#HRN={d}'] = d == hour_name\n",
    "\n",
    "        # 응답 이력 기반 피처: 지난 3시간 내 (예시) 응답 중 긍정/부정 응답 비율\n",
    "        for w_key, w_val in window_label.items():\n",
    "            w_val = w_val(t) if isinstance(w_val, Callable) else w_val\n",
    "            try:\n",
    "                v = label.loc[t - td(seconds=w_val):t]\n",
    "                if len(label_values) <= 2:\n",
    "                    row[f'ESM#LIK#{w_key}'] = np.sum(v == label_values[0]) / len(v) if len(v) > 0 else 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK={l}#{w_key}'] = np.sum(v == l) / len(v) if len(v) > 0 else 0\n",
    "            except (KeyError, IndexError):\n",
    "                if len(label_values) <= 2: \n",
    "                    row[f'ESM#LIK#{w_key}'] = 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK={l}#{w_key}'] = 0\n",
    "\n",
    "        row = {\n",
    "            k: 0.0 if _safe_na_check(v) else v\n",
    "            for k, v in row.items()\n",
    "        }\n",
    "        X.append(row)\n",
    "        y.append(label_cur)\n",
    "        date_times.append(timestamp)\n",
    "\n",
    "    log(f\"Complete feature extraction on {pid}'s data ({time.time() - _s:.2f} s).\")\n",
    "    X, y, group, date_times = pd.DataFrame(X), np.asarray(y), np.repeat(pid, len(y)), np.asarray(date_times)\n",
    "    \n",
    "    #출력 결과 코드\n",
    "    print(f\"[{pid}] Extracted {X.shape[0]} samples with {X.shape[1]} features\")\n",
    "    return X, y, group, date_times\n",
    "\n",
    "# _extract를 참가자 리스트 전체에 병렬/순차적으로 적용 -> 최종 feature 데이터\n",
    "# 출력: \n",
    "# X: 전체 참가자의 feature 데이터 (DataFrame)\n",
    "# y: 전체 라벨\n",
    "# group: 각 행에 해당하는 참가자의 ID\n",
    "# t_norm: timestamp를 기준 시점으로부터 정규화한 시간 (초 단위), XGB, NN에 사용 가능\n",
    "# date_times: 각 sample의 실제 timestamp (Datetime array)\n",
    "\n",
    "def extract(\n",
    "        pids: Iterable[str], \n",
    "        data: Dict[str, pd.Series], # 전체 센서 데이터\n",
    "        label: pd.Series, # 전체 라벨\n",
    "        label_values: List[str],\n",
    "        window_data: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "        window_label: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],        \n",
    "        categories: Dict[str, Optional[List[any]]] = None,        \n",
    "        resample_s: Dict[str, float] = None,\n",
    "        with_ray: bool=False\n",
    "):\n",
    "    if with_ray and not ray.is_initialized():\n",
    "        raise EnvironmentError('Ray should be initialized if \"with_ray\" is set as True.')\n",
    "\n",
    "    func = ray.remote(_extract).remote if with_ray else _extract\n",
    "    jobs = []\n",
    "    \n",
    "    # 참가자별로 _extract 실행 (단일 or 병렬)\n",
    "    for pid in pids:\n",
    "        d = dict()\n",
    "        for k, v in data.items():\n",
    "            try:\n",
    "                d[k] = v.loc[(pid, )]\n",
    "            except (KeyError, IndexError):\n",
    "                pass\n",
    "\n",
    "        job = func(\n",
    "            pid=pid,\n",
    "            data=d,\n",
    "            label=label.loc[(pid, )],\n",
    "            label_values=label_values,\n",
    "            window_data=window_data,\n",
    "            window_label=window_label,\n",
    "            categories=categories,\n",
    "            resample_s=resample_s\n",
    "        )\n",
    "        jobs.append(job)\n",
    "\n",
    "    jobs = ray.get(jobs) if with_ray else jobs\n",
    "\n",
    "    X = pd.concat([x for x, _, _, _ in jobs], axis=0, ignore_index=True)\n",
    "    y = np.concatenate([x for _, x, _, _ in jobs], axis=0)\n",
    "    group = np.concatenate([x for _, _, x, _ in jobs], axis=0)\n",
    "    date_times = np.concatenate([x for _, _, _, x in jobs], axis=0)\n",
    "    \n",
    "    # 모든 참가자의 feature / label 결과 통합. timestamp를 자정 기준으로 정렬 -> t_norm(상대적 시간) 계산 위한 준비\n",
    "    t_s = date_times.min().normalize().timestamp()\n",
    "    # datetime을 수치화된 상대 시간으로 변환. 머신러닝의 효과적인 학습 위함. t_s(시작 시간)기준으로 얼마나 지났는지\n",
    "    t_norm = np.asarray(list(map(lambda x: x.timestamp() - t_s, date_times)))\n",
    "\n",
    "    # 결측치 처리 및 dtype 설정\n",
    "    C, DTYPE = X.columns, X.dtypes\n",
    "\n",
    "    X = X.fillna({\n",
    "        **{c: False for c in C[(DTYPE == object) | (DTYPE == bool)]},\n",
    "        **{c: 0.0 for c in C[(DTYPE != object) & (DTYPE != bool)]},\n",
    "    }).astype({\n",
    "        **{c: 'bool' for c in C[(DTYPE == object) | (DTYPE == bool)]},\n",
    "        **{c: 'float32' for c in C[(DTYPE != object) & (DTYPE != bool)]},\n",
    "    })\n",
    "\n",
    "    return X, y, group, t_norm, date_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_VALUES = [1, 0]\n",
    "\n",
    "WINDOW_DATA = {\n",
    "    'S30': 30,\n",
    "    'M01': 60,\n",
    "    'M05': 60 * 5,\n",
    "    'M10': 60 * 10,\n",
    "    'M30': 60 * 30,\n",
    "    'H01': 60 * 60,\n",
    "    # 'H02': 60 * 60 * 2, # 추가 \n",
    "    'H03': 60 * 60 * 3,\n",
    "    # 'H04': 60 * 60 * 4, # 추가 \n",
    "    # 'H05': 60 * 60 * 5, # 추가\n",
    "    'H06': 60 * 60 * 6\n",
    "}\n",
    "\n",
    "WINDOW_LABEL = {\n",
    "    'H06': 60 * 60 * 6,\n",
    "    'H12': 60 * 60 * 12,\n",
    "    'H24': 60 * 60 * 24,\n",
    "}\n",
    "\n",
    "RESAMPLE_s = {\n",
    "    'EDA': 0.25,\n",
    "}\n",
    "\n",
    "DATA = load(os.path.join(PATH_INTERMEDIATE, 'proc.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:20:34,701\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "/var/folders/nk/s4x3th851w574ph7qygky7xc0000gn/T/ipykernel_83879/101961088.py:233: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P01's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P02's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P03's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P05's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84020)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P12's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P13's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84025)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P08's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P09's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P06's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P10's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P15's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [25-06-03 23:20:37] Begin feature extraction on P19's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [25-06-03 23:22:30] Complete feature extraction on P15's data (113.35 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [P15] Extracted 819 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [25-06-03 23:22:31] Begin feature extraction on P21's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [25-06-03 23:22:37] Complete feature extraction on P01's data (120.26 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [P01] Extracted 840 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [25-06-03 23:22:37] Begin feature extraction on P23's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [25-06-03 23:22:52] Complete feature extraction on P03's data (135.16 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [P03] Extracted 924 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [25-06-03 23:22:52] Begin feature extraction on P26's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84020)\u001b[0m [25-06-03 23:22:55] Complete feature extraction on P12's data (138.54 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84020)\u001b[0m [P12] Extracted 945 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84020)\u001b[0m [25-06-03 23:22:55] Begin feature extraction on P28's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [25-06-03 23:23:01] Complete feature extraction on P10's data (144.21 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [P10] Extracted 987 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [25-06-03 23:23:01] Begin feature extraction on P30's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [25-06-03 23:23:08] Complete feature extraction on P06's data (151.20 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [P06] Extracted 1029 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [25-06-03 23:23:08] Begin feature extraction on P31's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [25-06-03 23:23:14] Complete feature extraction on P05's data (157.57 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [P05] Extracted 1071 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [25-06-03 23:23:14] Begin feature extraction on P32's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [25-06-03 23:23:20] Complete feature extraction on P02's data (163.55 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [P02] Extracted 1071 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [25-06-03 23:23:20] Begin feature extraction on P33's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [25-06-03 23:23:26] Complete feature extraction on P13's data (168.82 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [P13] Extracted 1134 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [25-06-03 23:23:26] Begin feature extraction on P35's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [25-06-03 23:23:26] Complete feature extraction on P19's data (168.94 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [P19] Extracted 1092 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [25-06-03 23:23:26] Begin feature extraction on P39's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [25-06-03 23:23:30] Complete feature extraction on P09's data (173.59 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [P09] Extracted 1155 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [25-06-03 23:23:31] Begin feature extraction on P40's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84025)\u001b[0m [25-06-03 23:24:11] Complete feature extraction on P08's data (214.39 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84025)\u001b[0m [P08] Extracted 1449 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84025)\u001b[0m [25-06-03 23:24:11] Begin feature extraction on P42's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [25-06-03 23:25:09] Complete feature extraction on P21's data (158.67 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [P21] Extracted 1050 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [25-06-03 23:25:09] Begin feature extraction on P45's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [25-06-03 23:25:12] Complete feature extraction on P23's data (154.82 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [P23] Extracted 1029 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [25-06-03 23:25:12] Begin feature extraction on P47's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [25-06-03 23:25:32] Complete feature extraction on P33's data (131.30 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [P33] Extracted 819 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [25-06-03 23:25:32] Begin feature extraction on P48's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [25-06-03 23:25:57] Complete feature extraction on P31's data (168.83 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [P31] Extracted 1113 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [25-06-03 23:25:57] Begin feature extraction on P49's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [25-06-03 23:25:59] Complete feature extraction on P32's data (164.87 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [P32] Extracted 1092 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [25-06-03 23:25:59] Begin feature extraction on P50's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [25-06-03 23:26:08] Complete feature extraction on P35's data (162.09 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [P35] Extracted 1071 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [25-06-03 23:26:08] Begin feature extraction on P51's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [25-06-03 23:26:31] Complete feature extraction on P40's data (180.36 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [P40] Extracted 1197 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [25-06-03 23:26:31] Begin feature extraction on P52's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [25-06-03 23:26:35] Complete feature extraction on P39's data (189.26 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [P39] Extracted 1260 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [25-06-03 23:26:36] Begin feature extraction on P53's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [25-06-03 23:26:36] Complete feature extraction on P30's data (214.88 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [P30] Extracted 1470 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [25-06-03 23:26:36] Begin feature extraction on P55's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [25-06-03 23:26:48] Complete feature extraction on P26's data (236.03 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [P26] Extracted 1596 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [25-06-03 23:26:48] Begin feature extraction on P57's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [25-06-03 23:27:05] Complete feature extraction on P45's data (115.35 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [P45] Extracted 798 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [25-06-03 23:27:05] Begin feature extraction on P60's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84020)\u001b[0m [25-06-03 23:27:15] Complete feature extraction on P28's data (259.31 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84020)\u001b[0m [P28] Extracted 1743 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84020)\u001b[0m [25-06-03 23:27:15] Begin feature extraction on P61's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [25-06-03 23:27:19] Complete feature extraction on P48's data (106.87 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [P48] Extracted 777 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [25-06-03 23:27:19] Begin feature extraction on P66's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84025)\u001b[0m [25-06-03 23:27:44] Complete feature extraction on P42's data (213.11 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84025)\u001b[0m [P42] Extracted 1470 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84025)\u001b[0m [25-06-03 23:27:45] Begin feature extraction on P67's data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [25-06-03 23:28:07] Complete feature extraction on P50's data (127.50 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [P50] Extracted 903 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [25-06-03 23:28:07] Begin feature extraction on P69's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [25-06-03 23:28:50] Complete feature extraction on P52's data (139.42 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [P52] Extracted 903 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [25-06-03 23:28:51] Begin feature extraction on P70's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [25-06-03 23:28:57] Complete feature extraction on P51's data (169.40 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [P51] Extracted 1323 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [25-06-03 23:28:58] Begin feature extraction on P72's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [25-06-03 23:29:09] Complete feature extraction on P47's data (237.11 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [P47] Extracted 1617 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [25-06-03 23:29:09] Begin feature extraction on P75's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [25-06-03 23:29:29] Complete feature extraction on P55's data (172.97 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [P55] Extracted 1176 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [25-06-03 23:29:29] Begin feature extraction on P76's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [25-06-03 23:29:37] Complete feature extraction on P57's data (168.96 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [P57] Extracted 1197 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [25-06-03 23:29:37] Begin feature extraction on P77's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [25-06-03 23:29:53] Complete feature extraction on P49's data (236.37 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [P49] Extracted 1659 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [25-06-03 23:29:54] Begin feature extraction on P78's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [25-06-03 23:29:58] Complete feature extraction on P66's data (159.19 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [P66] Extracted 1092 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [25-06-03 23:29:58] Begin feature extraction on P79's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [25-06-03 23:30:04] Complete feature extraction on P53's data (208.35 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [P53] Extracted 1596 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [25-06-03 23:30:04] Begin feature extraction on P80's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [25-06-03 23:30:15] Complete feature extraction on P60's data (190.47 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84018)\u001b[0m [P60] Extracted 1281 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [25-06-03 23:30:38] Complete feature extraction on P70's data (107.29 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84022)\u001b[0m [P70] Extracted 777 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84020)\u001b[0m [25-06-03 23:30:53] Complete feature extraction on P61's data (218.03 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84020)\u001b[0m [P61] Extracted 1533 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84025)\u001b[0m [25-06-03 23:30:59] Complete feature extraction on P67's data (193.98 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84025)\u001b[0m [P67] Extracted 1365 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [25-06-03 23:31:26] Complete feature extraction on P79's data (87.95 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84027)\u001b[0m [P79] Extracted 756 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [25-06-03 23:31:26] Complete feature extraction on P77's data (108.81 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84028)\u001b[0m [P77] Extracted 924 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [25-06-03 23:31:31] Complete feature extraction on P69's data (203.68 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84026)\u001b[0m [P69] Extracted 1596 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [25-06-03 23:31:37] Complete feature extraction on P80's data (92.56 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84019)\u001b[0m [P80] Extracted 987 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [25-06-03 23:31:37] Complete feature extraction on P72's data (159.56 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84021)\u001b[0m [P72] Extracted 1365 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [25-06-03 23:31:47] Complete feature extraction on P78's data (113.77 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84024)\u001b[0m [P78] Extracted 1050 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [25-06-03 23:31:52] Complete feature extraction on P76's data (142.18 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84023)\u001b[0m [P76] Extracted 1365 samples with 84 features\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [25-06-03 23:31:52] Complete feature extraction on P75's data (162.75 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=84029)\u001b[0m [P75] Extracted 1533 samples with 84 features\n"
     ]
    }
   ],
   "source": [
    "with on_ray(num_cpus=12):\n",
    "    l = 'attention'\n",
    "\n",
    "    labels = LABELS_PROC[f'{l}_bin']\n",
    "    pids = labels.index.get_level_values('pcode').unique()\n",
    "\n",
    "    feat = extract(\n",
    "        pids=pids, \n",
    "        data=DATA,         \n",
    "        label=labels,\n",
    "        label_values=LABEL_VALUES,\n",
    "        window_data=WINDOW_DATA,\n",
    "        window_label=WINDOW_LABEL,\n",
    "        resample_s=RESAMPLE_s,\n",
    "        with_ray=True\n",
    "    )\n",
    "\n",
    "    dump(feat, os.path.join(PATH_INTERMEDIATE, f'{l}.pkl'))\n",
    "\n",
    "# 결과 timestamp 기반 응답 수가 35~83 -> 참가자 간 f1-score의 분산이 큰 경우, 조정 필요\n",
    "# Extracted 54 samples with 279 features: 응답수 54개, feature 279개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# attention\n",
      "- Feature space: 84; Cat.: 16; Num.: 68\n",
      "- Label distribution: (array([0, 1]), array([27552, 27447]))\n",
      "# attention feature extraction summary\n",
      "- Feature matrix: X.shape = (54999, 84)  (rows: 54999, features: 84)\n",
      "- Label vector:   y.shape = (54999,)\n",
      "- Group vector:   group.shape = (54999,)\n",
      "- Time norm:      t.shape = (54999,)\n",
      "- Label distribution: {0: 27552, 1: 27447}\n",
      "- Feature types: Cat. = 16, Num. = 68\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# attention만 대상\n",
    "X, y, group, t, _ = load(os.path.join(PATH_INTERMEDIATE, 'attention.pkl'))\n",
    "\n",
    "print(f'# attention')\n",
    "# categorical feature은 시간 기반 정보에서 생성. ESM 응답 이력 + 시간대\n",
    "print(f'- Feature space: {len(X.dtypes)}; Cat.: {np.sum(X.dtypes == bool)}; Num.: {np.sum(X.dtypes != bool)}')\n",
    "print(f'- Label distribution: {np.unique(y, return_counts=True)}')\n",
    "\n",
    "print(\"# attention feature extraction summary\")\n",
    "print(f\"- Feature matrix: X.shape = {X.shape}  (rows: {X.shape[0]}, features: {X.shape[1]})\")\n",
    "print(f\"- Label vector:   y.shape = {y.shape}\")\n",
    "print(f\"- Group vector:   group.shape = {group.shape}\")\n",
    "print(f\"- Time norm:      t.shape = {t.shape}\")\n",
    "\n",
    "print(f\"- Label distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "print(f\"- Feature types: Cat. = {np.sum(X.dtypes == bool)}, Num. = {np.sum(X.dtypes != bool)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the number of features is same as intented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_TIM: 16\n",
      "N_VAL_NUM: 1\n",
      "N_WIN_NUM: 64\n",
      "N_LBL: 3\n",
      "N_FEAT: 84\n"
     ]
    }
   ],
   "source": [
    "N_NUM, N_CAT_B, N_CAT_NB = 0, 0, 0 \n",
    "\n",
    "for k, v in DATA.items():\n",
    "    N_NUM = N_NUM + 1\n",
    "\n",
    "# Features relavant to delivery time\n",
    "N_TIM = 7 + 2 + 7\n",
    "print(f'N_TIM: {N_TIM}')\n",
    "        \n",
    "# Features relevant to latest value\n",
    "N_VAL_NUM = N_NUM\n",
    "print(f'N_VAL_NUM: {N_VAL_NUM}')\n",
    "\n",
    "# Features from time-windows\n",
    "N_WIN_NUM = N_NUM * 8 * len(WINDOW_DATA)\n",
    "\n",
    "print(f'N_WIN_NUM: {N_WIN_NUM}')\n",
    "\n",
    "\n",
    "# Features from previous labels\n",
    "N_LBL = len(WINDOW_LABEL) * (1 if len(LABEL_VALUES) <= 2 else len(LABEL_VALUES))\n",
    "print(f'N_LBL: {N_LBL}')\n",
    "\n",
    "N_FEAT = N_TIM + N_WIN_NUM + N_VAL_NUM + N_LBL\n",
    "print(f'N_FEAT: {N_FEAT}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, features are extracted as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "#stratifiedgroupkfold 추가\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import time\n",
    "import ray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "\n",
    "\n",
    "# logo의 단점: 참가자별 attention label의 불균형을 해결하기 위한 stratification 불가\n",
    "# StratifiedKFold 혹은 StratifiedGroupKFold 필요\n",
    "# StratifiedKFold는 한 pcode의 데이터가 train-test 모두에 포함이 가능 -> StratifiedGroupKFold\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'stratifiedgroupkfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = StratifiedGroupKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedGroupKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    normalize: bool = False,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    categories: Union[List, Dict[str, Dict[int, str]]] = None\n",
    "):\n",
    "    @contextmanager\n",
    "    def _log(task_type: str):\n",
    "        log(f'In progress: {task_type}.')\n",
    "        _t = time.time()\n",
    "        _err = None\n",
    "        _result = dict()\n",
    "        \n",
    "        try:\n",
    "            yield _result\n",
    "        except:\n",
    "            _err = tb.format_exc()\n",
    "        finally:\n",
    "            _e = time.time() - _t\n",
    "            if _err:\n",
    "                _msg = f'Failure: {task_type} ({_e:.2f}s). Keep running without this task. Caused by: \\n{_err}' \n",
    "            else:\n",
    "                _msg = f'Success: {task_type} ({_e:.2f}s).' \n",
    "                if _result:\n",
    "                    _r = '\\n'.join([f'- {k}: {v}' for k, v in _result.items()])\n",
    "                    _msg = f'{_msg}\\n{_r}'\n",
    "            log(_msg)\n",
    "    \n",
    "    if normalize:\n",
    "        with _log(f'[{name}] Normalizing numeric features'):\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "         \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "           \n",
    "    if select:\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "            \n",
    "        for i, s in enumerate(select):\n",
    "            with _log(f'[{name}] {i+1}-th Feature selection') as r:\n",
    "                C = np.asarray(X_train.columns)\n",
    "                r['# Orig. Feat.'] = f'{len(C)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                r['# Sel. Feat.'] = f'{len(C_sel)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "\n",
    "    if oversample:\n",
    "        with _log('Oversampling'):\n",
    "            if len(C_cat):\n",
    "                cat_mask = np.isin(X_train.columns, C_cat)\n",
    "                cat_indices = np.where(cat_mask)[0]  # <- 인덱스 리스트로 변환\n",
    "                sampler = SMOTENC(categorical_features=cat_indices.tolist(), random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    with _log(f'[{name}] Training'):\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "        result = FoldResult(\n",
    "            name=name,\n",
    "            estimator=estimator,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            categories=categories\n",
    "        )\n",
    "        dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    \n",
    "\n",
    "# def cross_val(\n",
    "#     X: pd.DataFrame,\n",
    "#     y: np.ndarray,\n",
    "#     groups: np.ndarray,\n",
    "#     path: str,\n",
    "#     name: str,\n",
    "#     estimator: BaseEstimator,\n",
    "#     categories: List[str] = None,\n",
    "#     normalize: bool = False,\n",
    "#     split: str = None,\n",
    "#     split_params: Dict[str, any] = None,\n",
    "#     select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "#     oversample: bool = False,\n",
    "#     random_state: int = None\n",
    "# ):\n",
    "\n",
    "#StratifiedKFold\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = f\"fold{idx_fold + 1}\"\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            categories=categories,\n",
    "            estimator=clone(estimator),\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor Modification on XGBClassifer\n",
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고) xgboost 공홈 주소 \n",
    "- https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier\n",
    "- https://xgboost.readthedocs.io/en/release_2.0.0/parameter.html\n",
    "\n",
    "### 참고) RF 공홈 주소 \n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "<br> parameter 수정 시 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: xgboost\r\n",
      "Version: 2.1.4\r\n",
      "Summary: XGBoost Python Package\r\n",
      "Home-page: \r\n",
      "Author: \r\n",
      "Author-email: Hyunsu Cho <chohyu01@cs.washington.edu>, Jiaming Yuan <jm.yuan@outlook.com>\r\n",
      "License: Apache-2.0\r\n",
      "Location: /Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages\r\n",
      "Requires: numpy, scipy\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, our feature data has a big-$p$, little-$N$ problem: # sample = 2,619 while # features = 3,356.\n",
    "Therefore, we need to choose important features only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:31:58,959\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P01' 'P01' 'P01' ... 'P80' 'P80' 'P80']\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:01] In progress: [logreg#fold4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:01] Success: [logreg#fold4] Normalizing numeric features (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:01] In progress: [logreg#fold4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:01] In progress: [logreg#fold2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:01] Success: [logreg#fold2] Normalizing numeric features (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:01] In progress: [logreg#fold2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:01] In progress: [logreg#fold3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:01] Success: [logreg#fold3] Normalizing numeric features (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:01] In progress: [logreg#fold3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:01] In progress: [logreg#fold5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:01] Success: [logreg#fold5] Normalizing numeric features (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:01] In progress: [logreg#fold5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:01] In progress: [logreg#fold1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:01] Success: [logreg#fold1] Normalizing numeric features (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:01] In progress: [logreg#fold1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:03] Success: [logreg#fold1] Training (1.35s).\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:03] Success: [logreg#fold4] Training (1.54s).\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:03] Success: [logreg#fold5] Training (1.57s).\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:03] Success: [logreg#fold3] Training (1.77s).\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:04] Success: [logreg#fold2] Training (2.50s).\n",
      "['P01' 'P01' 'P01' ... 'P80' 'P80' 'P80']\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:04] In progress: [rf_ns#fold1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:04] Success: [rf_ns#fold1] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:04] In progress: [rf_ns#fold1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:04] In progress: [rf_ns#fold2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:04] Success: [rf_ns#fold2] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:04] In progress: [rf_ns#fold2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:04] In progress: [rf_ns#fold3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:04] In progress: [rf_ns#fold4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:04] Success: [rf_ns#fold4] Normalizing numeric features (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:04] In progress: [rf_ns#fold4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:04] Success: [rf_ns#fold3] Normalizing numeric features (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:04] In progress: [rf_ns#fold3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:04] In progress: [rf_ns#fold5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:04] Success: [rf_ns#fold5] Normalizing numeric features (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:04] In progress: [rf_ns#fold5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:05] Success: [rf_ns#fold4] 1-th Feature selection (0.77s).\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m - # Sel. Feat.: 63 (# Cat. = 8; # Num. = 55)\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:05] In progress: [rf_ns#fold4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:05] Success: [rf_ns#fold1] 1-th Feature selection (0.87s).\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 10; # Num. = 48)\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:05] In progress: [rf_ns#fold1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:05] Success: [rf_ns#fold2] 1-th Feature selection (0.94s).\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 6; # Num. = 52)\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:05] In progress: [rf_ns#fold2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:05] Success: [rf_ns#fold3] 1-th Feature selection (0.87s).\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m - # Sel. Feat.: 62 (# Cat. = 12; # Num. = 50)\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:05] In progress: [rf_ns#fold3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:05] Success: [rf_ns#fold5] 1-th Feature selection (0.76s).\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 7; # Num. = 51)\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:05] In progress: [rf_ns#fold5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:12] Success: [rf_ns#fold3] Training (6.99s).\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:12] Success: [rf_ns#fold1] Training (7.34s).\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:13] Success: [rf_ns#fold4] Training (7.86s).\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:13] Success: [rf_ns#fold2] Training (7.87s).\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:13] Success: [rf_ns#fold5] Training (8.01s).\n",
      "['P01' 'P01' 'P01' ... 'P80' 'P80' 'P80']\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:13] In progress: [xgb_ns#fold2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:13] In progress: [xgb_ns#fold1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:13] Success: [xgb_ns#fold1] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:13] In progress: [xgb_ns#fold1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:14] Success: [xgb_ns#fold3] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:14] Success: [xgb_ns#fold2] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:14] Success: [xgb_ns#fold4] Normalizing numeric features (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:14] Success: [xgb_ns#fold5] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:14] Success: [xgb_ns#fold1] 1-th Feature selection (0.78s).\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 10; # Num. = 48)\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:14] Success: [xgb_ns#fold3] 1-th Feature selection (0.81s).\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m - # Sel. Feat.: 62 (# Cat. = 12; # Num. = 50)\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:14] Success: [xgb_ns#fold4] 1-th Feature selection (0.80s).\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m - # Sel. Feat.: 63 (# Cat. = 8; # Num. = 55)\n",
      "\u001b[2m\u001b[36m(_train pid=87288)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:14] Success: [xgb_ns#fold5] 1-th Feature selection (0.74s).\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 7; # Num. = 51)\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold5] Training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:14] Success: [xgb_ns#fold2] 1-th Feature selection (0.97s).\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 6; # Num. = 52)\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:14] In progress: [xgb_ns#fold2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87294)\u001b[0m [25-06-03 23:32:16] Success: [xgb_ns#fold1] Training (1.57s).\n",
      "\u001b[2m\u001b[36m(_train pid=87308)\u001b[0m [25-06-03 23:32:16] Success: [xgb_ns#fold3] Training (1.59s).\n",
      "\u001b[2m\u001b[36m(_train pid=87298)\u001b[0m [25-06-03 23:32:16] Success: [xgb_ns#fold2] Training (1.49s).\n",
      "\u001b[2m\u001b[36m(_train pid=87302)\u001b[0m [25-06-03 23:32:16] Success: [xgb_ns#fold5] Training (1.60s).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_LOGREG = LogisticRegression(\n",
    "    penalty='l1', solver='liblinear', random_state=RANDOM_STATE\n",
    ")\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE, max_depth=5)\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "    estimator=LinearSVC(\n",
    "        penalty='l1',\n",
    "        loss='squared_hinge',\n",
    "        dual=False,\n",
    "        tol=1e-3,\n",
    "        C=1e-2,\n",
    "        max_iter=5000,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    threshold=1e-5\n",
    ")\n",
    "\n",
    "CLS = ['attention']\n",
    "SETTINGS = [\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_LOGREG),  # 여기서 logistic으로 대체\n",
    "        oversample=False,\n",
    "        select=None,\n",
    "        name='logreg'\n",
    "    ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_ns'\n",
    "    ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_RF),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='rf_os'\n",
    "#     ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_ns'\n",
    "    ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_XGB),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='xgb_os'\n",
    "#     )\n",
    "]\n",
    "\n",
    "with on_ray(num_cpus=12):\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):\n",
    "        p = os.path.join(PATH_INTERMEDIATE, f'{l}.pkl')\n",
    "        par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', l)\n",
    "        os.makedirs(par_dir, exist_ok=True)\n",
    "        \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        print(groups)\n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups,\n",
    "            path=par_dir,\n",
    "            categories=cats,\n",
    "            normalize=True,\n",
    "            split='stratifiedgroupkfold',\n",
    "            split_params={'n_splits': 5}, # logo시 삭제\n",
    "            random_state=RANDOM_STATE,\n",
    "            **s\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # StratifiedGroupKFold + StratifiedKFold 비교용 실험 확장\n",
    "# SETTINGS_EXTENDED = []\n",
    "# for s in SETTINGS:\n",
    "#     # StratifiedGroupKFold 설정\n",
    "#     s_group = dict(s)\n",
    "#     s_group['split'] = 'stratifiedgroupkfold'\n",
    "#     s_group['name'] = s_group['name']\n",
    "#     SETTINGS_EXTENDED.append(s_group)\n",
    "\n",
    "#     # StratifiedKFold 설정\n",
    "#     s_kfold = dict(s)\n",
    "#     s_kfold['split'] = 'kfold'\n",
    "#     s_kfold['name'] = s_kfold['name'] + '_kfold'\n",
    "#     SETTINGS_EXTENDED.append(s_kfold)\n",
    "\n",
    "# with on_ray(num_cpus=12):\n",
    "#     for l, s in product(CLS, SETTINGS_EXTENDED):\n",
    "#         p = os.path.join(PATH_INTERMEDIATE, f'{l}.pkl')\n",
    "#         par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', l)\n",
    "#         os.makedirs(par_dir, exist_ok=True)\n",
    "\n",
    "#         X, y, groups, t, datetimes = load(p)\n",
    "#         cats = X.columns[X.dtypes == bool]\n",
    "\n",
    "#         split_mode = s['split']\n",
    "#         split_params = {'n_splits': 5}\n",
    "\n",
    "#         # 중복 방지: s 딕셔너리에서 'split' 키 제거\n",
    "#         s_clean = {k: v for k, v in s.items() if k != 'split'}\n",
    "\n",
    "#         cross_val(\n",
    "#             X=X, y=y, groups=groups,\n",
    "#             path=par_dir,\n",
    "#             categories=cats,\n",
    "#             normalize=True,\n",
    "#             split=split_mode,\n",
    "#             split_params=split_params,\n",
    "#             random_state=RANDOM_STATE,\n",
    "#             **s_clean\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "  Train groups: ['P02' 'P05' 'P09' 'P10' 'P12' 'P13' 'P15' 'P19' 'P21' 'P23' 'P26' 'P31'\n",
      " 'P32' 'P33' 'P35' 'P39' 'P40' 'P42' 'P45' 'P47' 'P48' 'P50' 'P51' 'P52'\n",
      " 'P53' 'P55' 'P57' 'P60' 'P66' 'P67' 'P69' 'P70' 'P72' 'P75' 'P77' 'P78'\n",
      " 'P79' 'P80']\n",
      "  Test groups:  ['P01' 'P03' 'P06' 'P08' 'P28' 'P30' 'P49' 'P61' 'P76']\n",
      "OK: No overlap between train/test groups\n",
      "\n",
      "Fold 2\n",
      "  Train groups: ['P01' 'P02' 'P03' 'P06' 'P08' 'P13' 'P15' 'P19' 'P21' 'P23' 'P26' 'P28'\n",
      " 'P30' 'P31' 'P32' 'P33' 'P35' 'P39' 'P40' 'P42' 'P45' 'P47' 'P49' 'P50'\n",
      " 'P51' 'P52' 'P53' 'P55' 'P57' 'P61' 'P67' 'P70' 'P72' 'P76' 'P78' 'P79'\n",
      " 'P80']\n",
      "  Test groups:  ['P05' 'P09' 'P10' 'P12' 'P48' 'P60' 'P66' 'P69' 'P75' 'P77']\n",
      "OK: No overlap between train/test groups\n",
      "\n",
      "Fold 3\n",
      "  Train groups: ['P01' 'P03' 'P05' 'P06' 'P08' 'P09' 'P10' 'P12' 'P13' 'P15' 'P23' 'P28'\n",
      " 'P30' 'P31' 'P33' 'P40' 'P42' 'P45' 'P47' 'P48' 'P49' 'P50' 'P51' 'P52'\n",
      " 'P55' 'P57' 'P60' 'P61' 'P66' 'P69' 'P70' 'P72' 'P75' 'P76' 'P77' 'P78'\n",
      " 'P79' 'P80']\n",
      "  Test groups:  ['P02' 'P19' 'P21' 'P26' 'P32' 'P35' 'P39' 'P53' 'P67']\n",
      "OK: No overlap between train/test groups\n",
      "\n",
      "Fold 4\n",
      "  Train groups: ['P01' 'P02' 'P03' 'P05' 'P06' 'P08' 'P09' 'P10' 'P12' 'P19' 'P21' 'P26'\n",
      " 'P28' 'P30' 'P32' 'P33' 'P35' 'P39' 'P47' 'P48' 'P49' 'P51' 'P52' 'P53'\n",
      " 'P55' 'P60' 'P61' 'P66' 'P67' 'P69' 'P72' 'P75' 'P76' 'P77' 'P78' 'P79'\n",
      " 'P80']\n",
      "  Test groups:  ['P13' 'P15' 'P23' 'P31' 'P40' 'P42' 'P45' 'P50' 'P57' 'P70']\n",
      "OK: No overlap between train/test groups\n",
      "\n",
      "Fold 5\n",
      "  Train groups: ['P01' 'P02' 'P03' 'P05' 'P06' 'P08' 'P09' 'P10' 'P12' 'P13' 'P15' 'P19'\n",
      " 'P21' 'P23' 'P26' 'P28' 'P30' 'P31' 'P32' 'P35' 'P39' 'P40' 'P42' 'P45'\n",
      " 'P48' 'P49' 'P50' 'P53' 'P57' 'P60' 'P61' 'P66' 'P67' 'P69' 'P70' 'P75'\n",
      " 'P76' 'P77']\n",
      "  Test groups:  ['P33' 'P47' 'P51' 'P52' 'P55' 'P72' 'P78' 'P79' 'P80']\n",
      "OK: No overlap between train/test groups\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# StratifiedGroupKFold 잘 되었는지 확인\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(sgkf.split(X, y, groups=groups)):\n",
    "    train_groups = np.unique(groups[train_idx])\n",
    "    test_groups = np.unique(groups[test_idx])\n",
    "    intersection = np.intersect1d(train_groups, test_groups)\n",
    "    \n",
    "    print(f\"Fold {fold_idx + 1}\")\n",
    "    print(f\"  Train groups: {train_groups}\")\n",
    "    print(f\"  Test groups:  {test_groups}\")\n",
    "    \n",
    "    if len(intersection) == 0:\n",
    "        print(\"OK: No overlap between train/test groups\\n\")\n",
    "    else:\n",
    "        print(f\"Overlap found: {intersection}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "from itertools import product\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, \\\n",
    "    confusion_matrix, precision_recall_fscore_support, \\\n",
    "    roc_auc_score, matthews_corrcoef, average_precision_score, \\\n",
    "    log_loss, brier_score_loss\n",
    "import scipy.stats.mstats as ms\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_proba: np.ndarray,\n",
    "    classes: np.ndarray\n",
    ") -> Dict[str, any]:\n",
    "    R = {}\n",
    "    n_classes = len(classes)\n",
    "    is_multiclass = n_classes > 2\n",
    "    is_same_y = len(np.unique(y_true)) == 1\n",
    "    R['inst'] = len(y_true)\n",
    "    \n",
    "    for c in classes:\n",
    "        R[f'inst_{c}'] = np.sum(y_true == c)\n",
    "        \n",
    "    if not is_multiclass:\n",
    "        _, cnt = np.unique(y_true, return_counts=True)\n",
    "        \n",
    "        if len(cnt) > 1:\n",
    "            R['class_ratio'] = cnt[0] / cnt[1]\n",
    "        else:\n",
    "            R['class_ratio'] = np.nan\n",
    "\n",
    "    C = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=classes)\n",
    "    for (i1, c1), (i2, c2) in product(enumerate(classes), enumerate(classes)):\n",
    "        R[f'true_{c1}_pred_{c2}'] = C[i1, i2]\n",
    "\n",
    "    # Threshold Measure\n",
    "    R['acc'] = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['bac'] = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['gmean'] = ms.gmean(np.diag(C) / np.sum(C, axis=1))\n",
    "    R['mcc'] = matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    if is_multiclass:\n",
    "        for avg in ('macro', 'micro'):\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                labels=classes,\n",
    "                average=avg, \n",
    "                zero_division=0\n",
    "            )\n",
    "            R[f'pre_{avg}'] = pre\n",
    "            R[f'rec_{avg}'] = rec\n",
    "            R[f'f1_{avg}'] = f1\n",
    "    else:\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true=y_true, y_pred=y_pred, pos_label=c, average='macro', zero_division=0\n",
    "        )\n",
    "        R[f'pre_macro'] = pre\n",
    "        R[f'rec_macro'] = rec\n",
    "        R[f'f1_macro'] = f1\n",
    "        \n",
    "        for c in classes:\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true, y_pred=y_pred, pos_label=c, average='binary', zero_division=0\n",
    "            )\n",
    "            R[f'pre_{c}'] = pre\n",
    "            R[f'rec_{c}'] = rec\n",
    "            R[f'f1_{c}'] = f1\n",
    "\n",
    "    # Ranking Measure\n",
    "    if is_multiclass:\n",
    "        for avg, mc in product(('macro', 'micro'), ('ovr', 'ovo')):\n",
    "            R[f'roauc_{avg}_{mc}'] = roc_auc_score(\n",
    "                y_true=y_true, y_score=y_proba,\n",
    "                average=avg, multi_class=mc, labels=classes\n",
    "            ) if not is_same_y else np.nan\n",
    "    else:\n",
    "        R[f'roauc'] = roc_auc_score(\n",
    "            y_true=y_true, y_score=y_proba[:, 1], average=None\n",
    "        ) if not is_same_y else np.nan\n",
    "        for i, c in enumerate(classes):\n",
    "            R[f'prauc_{c}'] = average_precision_score(\n",
    "                y_true=y_true, y_score=y_proba[:, i], pos_label=c, average=None\n",
    "            ) \n",
    "            R[f'prauc_ref_{c}'] = np.sum(y_true == c) / len(y_true)\n",
    "\n",
    "    # Probability Measure\n",
    "    R['log_loss'] = log_loss(y_true=y_true, y_pred=y_proba, labels=classes, normalize=True)\n",
    "\n",
    "    if not is_multiclass:\n",
    "        R[f'brier_loss'] = brier_score_loss(\n",
    "            y_true=y_true, y_prob=y_proba[:, 1], pos_label=classes[1]\n",
    "        )\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n",
      "/Users/idong-won/anaconda3/envs/ray-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:3284: FutureWarning: y_prob was deprecated in version 1.5 and will be removed in 1.7.Please use ``y_proba`` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>split</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_inst</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>test_class_ratio</th>\n",
       "      <th>test_true_0_pred_0</th>\n",
       "      <th>test_true_0_pred_1</th>\n",
       "      <th>...</th>\n",
       "      <th>train_pre_1</th>\n",
       "      <th>train_rec_1</th>\n",
       "      <th>train_f1_1</th>\n",
       "      <th>train_roauc</th>\n",
       "      <th>train_prauc_0</th>\n",
       "      <th>train_prauc_ref_0</th>\n",
       "      <th>train_prauc_1</th>\n",
       "      <th>train_prauc_ref_1</th>\n",
       "      <th>train_log_loss</th>\n",
       "      <th>train_brier_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention</td>\n",
       "      <td>rf_ns</td>\n",
       "      <td>fold2</td>\n",
       "      <td>58</td>\n",
       "      <td>11361</td>\n",
       "      <td>6468</td>\n",
       "      <td>4893</td>\n",
       "      <td>1.321888</td>\n",
       "      <td>4013</td>\n",
       "      <td>2455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.723702</td>\n",
       "      <td>0.939523</td>\n",
       "      <td>0.817610</td>\n",
       "      <td>0.895697</td>\n",
       "      <td>0.899057</td>\n",
       "      <td>0.483157</td>\n",
       "      <td>0.904547</td>\n",
       "      <td>0.516843</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.162952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attention</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>fold5</td>\n",
       "      <td>58</td>\n",
       "      <td>9996</td>\n",
       "      <td>4641</td>\n",
       "      <td>5355</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>3309</td>\n",
       "      <td>1332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997678</td>\n",
       "      <td>0.991943</td>\n",
       "      <td>0.994802</td>\n",
       "      <td>0.998526</td>\n",
       "      <td>0.998453</td>\n",
       "      <td>0.509099</td>\n",
       "      <td>0.998313</td>\n",
       "      <td>0.490901</td>\n",
       "      <td>0.053464</td>\n",
       "      <td>0.008207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attention</td>\n",
       "      <td>logreg</td>\n",
       "      <td>fold4</td>\n",
       "      <td>84</td>\n",
       "      <td>10437</td>\n",
       "      <td>4410</td>\n",
       "      <td>6027</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>3036</td>\n",
       "      <td>1374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722357</td>\n",
       "      <td>0.710924</td>\n",
       "      <td>0.716595</td>\n",
       "      <td>0.830485</td>\n",
       "      <td>0.853235</td>\n",
       "      <td>0.519321</td>\n",
       "      <td>0.814985</td>\n",
       "      <td>0.480679</td>\n",
       "      <td>0.506822</td>\n",
       "      <td>0.170054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attention</td>\n",
       "      <td>logreg</td>\n",
       "      <td>fold5</td>\n",
       "      <td>84</td>\n",
       "      <td>9996</td>\n",
       "      <td>4641</td>\n",
       "      <td>5355</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>3213</td>\n",
       "      <td>1428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744053</td>\n",
       "      <td>0.736239</td>\n",
       "      <td>0.740126</td>\n",
       "      <td>0.842762</td>\n",
       "      <td>0.855518</td>\n",
       "      <td>0.509099</td>\n",
       "      <td>0.833922</td>\n",
       "      <td>0.490901</td>\n",
       "      <td>0.493012</td>\n",
       "      <td>0.164236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>attention</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>fold4</td>\n",
       "      <td>63</td>\n",
       "      <td>10437</td>\n",
       "      <td>4410</td>\n",
       "      <td>6027</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>3069</td>\n",
       "      <td>1341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996861</td>\n",
       "      <td>0.993184</td>\n",
       "      <td>0.995019</td>\n",
       "      <td>0.998516</td>\n",
       "      <td>0.998240</td>\n",
       "      <td>0.519321</td>\n",
       "      <td>0.998555</td>\n",
       "      <td>0.480679</td>\n",
       "      <td>0.054519</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>attention</td>\n",
       "      <td>rf_ns</td>\n",
       "      <td>fold3</td>\n",
       "      <td>62</td>\n",
       "      <td>11193</td>\n",
       "      <td>5859</td>\n",
       "      <td>5334</td>\n",
       "      <td>1.098425</td>\n",
       "      <td>3709</td>\n",
       "      <td>2150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726300</td>\n",
       "      <td>0.917786</td>\n",
       "      <td>0.810892</td>\n",
       "      <td>0.887291</td>\n",
       "      <td>0.896706</td>\n",
       "      <td>0.495206</td>\n",
       "      <td>0.890616</td>\n",
       "      <td>0.504794</td>\n",
       "      <td>0.504753</td>\n",
       "      <td>0.163415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>attention</td>\n",
       "      <td>rf_ns</td>\n",
       "      <td>fold1</td>\n",
       "      <td>58</td>\n",
       "      <td>12012</td>\n",
       "      <td>6174</td>\n",
       "      <td>5838</td>\n",
       "      <td>1.057554</td>\n",
       "      <td>3618</td>\n",
       "      <td>2556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731809</td>\n",
       "      <td>0.916886</td>\n",
       "      <td>0.813960</td>\n",
       "      <td>0.890433</td>\n",
       "      <td>0.900773</td>\n",
       "      <td>0.497313</td>\n",
       "      <td>0.892242</td>\n",
       "      <td>0.502687</td>\n",
       "      <td>0.503643</td>\n",
       "      <td>0.162645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>attention</td>\n",
       "      <td>rf_ns</td>\n",
       "      <td>fold4</td>\n",
       "      <td>63</td>\n",
       "      <td>10437</td>\n",
       "      <td>4410</td>\n",
       "      <td>6027</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>2394</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701371</td>\n",
       "      <td>0.919281</td>\n",
       "      <td>0.795676</td>\n",
       "      <td>0.891413</td>\n",
       "      <td>0.908462</td>\n",
       "      <td>0.519321</td>\n",
       "      <td>0.883067</td>\n",
       "      <td>0.480679</td>\n",
       "      <td>0.514016</td>\n",
       "      <td>0.167794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>attention</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>fold3</td>\n",
       "      <td>62</td>\n",
       "      <td>11193</td>\n",
       "      <td>5859</td>\n",
       "      <td>5334</td>\n",
       "      <td>1.098425</td>\n",
       "      <td>4410</td>\n",
       "      <td>1449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997593</td>\n",
       "      <td>0.993533</td>\n",
       "      <td>0.995559</td>\n",
       "      <td>0.998491</td>\n",
       "      <td>0.998239</td>\n",
       "      <td>0.495206</td>\n",
       "      <td>0.998523</td>\n",
       "      <td>0.504794</td>\n",
       "      <td>0.053620</td>\n",
       "      <td>0.008060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>attention</td>\n",
       "      <td>logreg</td>\n",
       "      <td>fold2</td>\n",
       "      <td>84</td>\n",
       "      <td>11361</td>\n",
       "      <td>6468</td>\n",
       "      <td>4893</td>\n",
       "      <td>1.321888</td>\n",
       "      <td>5052</td>\n",
       "      <td>1416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744598</td>\n",
       "      <td>0.750111</td>\n",
       "      <td>0.747344</td>\n",
       "      <td>0.836093</td>\n",
       "      <td>0.837231</td>\n",
       "      <td>0.483157</td>\n",
       "      <td>0.842789</td>\n",
       "      <td>0.516843</td>\n",
       "      <td>0.500065</td>\n",
       "      <td>0.167397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>attention</td>\n",
       "      <td>logreg</td>\n",
       "      <td>fold3</td>\n",
       "      <td>84</td>\n",
       "      <td>11193</td>\n",
       "      <td>5859</td>\n",
       "      <td>5334</td>\n",
       "      <td>1.098425</td>\n",
       "      <td>4393</td>\n",
       "      <td>1466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741227</td>\n",
       "      <td>0.742233</td>\n",
       "      <td>0.741730</td>\n",
       "      <td>0.837645</td>\n",
       "      <td>0.843514</td>\n",
       "      <td>0.495206</td>\n",
       "      <td>0.839422</td>\n",
       "      <td>0.504794</td>\n",
       "      <td>0.498377</td>\n",
       "      <td>0.166622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>attention</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>fold2</td>\n",
       "      <td>58</td>\n",
       "      <td>11361</td>\n",
       "      <td>6468</td>\n",
       "      <td>4893</td>\n",
       "      <td>1.321888</td>\n",
       "      <td>4558</td>\n",
       "      <td>1910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997593</td>\n",
       "      <td>0.992108</td>\n",
       "      <td>0.994843</td>\n",
       "      <td>0.999087</td>\n",
       "      <td>0.998803</td>\n",
       "      <td>0.483157</td>\n",
       "      <td>0.999187</td>\n",
       "      <td>0.516843</td>\n",
       "      <td>0.051566</td>\n",
       "      <td>0.007906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>attention</td>\n",
       "      <td>rf_ns</td>\n",
       "      <td>fold5</td>\n",
       "      <td>58</td>\n",
       "      <td>9996</td>\n",
       "      <td>4641</td>\n",
       "      <td>5355</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>2666</td>\n",
       "      <td>1975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738290</td>\n",
       "      <td>0.896795</td>\n",
       "      <td>0.809860</td>\n",
       "      <td>0.894305</td>\n",
       "      <td>0.908259</td>\n",
       "      <td>0.509099</td>\n",
       "      <td>0.890876</td>\n",
       "      <td>0.490901</td>\n",
       "      <td>0.508796</td>\n",
       "      <td>0.164662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>attention</td>\n",
       "      <td>logreg</td>\n",
       "      <td>fold1</td>\n",
       "      <td>84</td>\n",
       "      <td>12012</td>\n",
       "      <td>6174</td>\n",
       "      <td>5838</td>\n",
       "      <td>1.057554</td>\n",
       "      <td>4427</td>\n",
       "      <td>1747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739244</td>\n",
       "      <td>0.745060</td>\n",
       "      <td>0.742141</td>\n",
       "      <td>0.838137</td>\n",
       "      <td>0.844389</td>\n",
       "      <td>0.497313</td>\n",
       "      <td>0.835707</td>\n",
       "      <td>0.502687</td>\n",
       "      <td>0.498520</td>\n",
       "      <td>0.166283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>fold1</td>\n",
       "      <td>58</td>\n",
       "      <td>12012</td>\n",
       "      <td>6174</td>\n",
       "      <td>5838</td>\n",
       "      <td>1.057554</td>\n",
       "      <td>4375</td>\n",
       "      <td>1799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997166</td>\n",
       "      <td>0.993151</td>\n",
       "      <td>0.995154</td>\n",
       "      <td>0.998711</td>\n",
       "      <td>0.998188</td>\n",
       "      <td>0.497313</td>\n",
       "      <td>0.998970</td>\n",
       "      <td>0.502687</td>\n",
       "      <td>0.050322</td>\n",
       "      <td>0.007572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label     alg  split  n_feature  test_inst  test_inst_0  test_inst_1  \\\n",
       "0   attention   rf_ns  fold2         58      11361         6468         4893   \n",
       "1   attention  xgb_ns  fold5         58       9996         4641         5355   \n",
       "2   attention  logreg  fold4         84      10437         4410         6027   \n",
       "3   attention  logreg  fold5         84       9996         4641         5355   \n",
       "4   attention  xgb_ns  fold4         63      10437         4410         6027   \n",
       "5   attention   rf_ns  fold3         62      11193         5859         5334   \n",
       "6   attention   rf_ns  fold1         58      12012         6174         5838   \n",
       "7   attention   rf_ns  fold4         63      10437         4410         6027   \n",
       "8   attention  xgb_ns  fold3         62      11193         5859         5334   \n",
       "9   attention  logreg  fold2         84      11361         6468         4893   \n",
       "10  attention  logreg  fold3         84      11193         5859         5334   \n",
       "11  attention  xgb_ns  fold2         58      11361         6468         4893   \n",
       "12  attention   rf_ns  fold5         58       9996         4641         5355   \n",
       "13  attention  logreg  fold1         84      12012         6174         5838   \n",
       "14  attention  xgb_ns  fold1         58      12012         6174         5838   \n",
       "\n",
       "    test_class_ratio  test_true_0_pred_0  test_true_0_pred_1  ...  \\\n",
       "0           1.321888                4013                2455  ...   \n",
       "1           0.866667                3309                1332  ...   \n",
       "2           0.731707                3036                1374  ...   \n",
       "3           0.866667                3213                1428  ...   \n",
       "4           0.731707                3069                1341  ...   \n",
       "5           1.098425                3709                2150  ...   \n",
       "6           1.057554                3618                2556  ...   \n",
       "7           0.731707                2394                2016  ...   \n",
       "8           1.098425                4410                1449  ...   \n",
       "9           1.321888                5052                1416  ...   \n",
       "10          1.098425                4393                1466  ...   \n",
       "11          1.321888                4558                1910  ...   \n",
       "12          0.866667                2666                1975  ...   \n",
       "13          1.057554                4427                1747  ...   \n",
       "14          1.057554                4375                1799  ...   \n",
       "\n",
       "    train_pre_1  train_rec_1  train_f1_1  train_roauc  train_prauc_0  \\\n",
       "0      0.723702     0.939523    0.817610     0.895697       0.899057   \n",
       "1      0.997678     0.991943    0.994802     0.998526       0.998453   \n",
       "2      0.722357     0.710924    0.716595     0.830485       0.853235   \n",
       "3      0.744053     0.736239    0.740126     0.842762       0.855518   \n",
       "4      0.996861     0.993184    0.995019     0.998516       0.998240   \n",
       "5      0.726300     0.917786    0.810892     0.887291       0.896706   \n",
       "6      0.731809     0.916886    0.813960     0.890433       0.900773   \n",
       "7      0.701371     0.919281    0.795676     0.891413       0.908462   \n",
       "8      0.997593     0.993533    0.995559     0.998491       0.998239   \n",
       "9      0.744598     0.750111    0.747344     0.836093       0.837231   \n",
       "10     0.741227     0.742233    0.741730     0.837645       0.843514   \n",
       "11     0.997593     0.992108    0.994843     0.999087       0.998803   \n",
       "12     0.738290     0.896795    0.809860     0.894305       0.908259   \n",
       "13     0.739244     0.745060    0.742141     0.838137       0.844389   \n",
       "14     0.997166     0.993151    0.995154     0.998711       0.998188   \n",
       "\n",
       "    train_prauc_ref_0  train_prauc_1  train_prauc_ref_1  train_log_loss  \\\n",
       "0            0.483157       0.904547           0.516843        0.504762   \n",
       "1            0.509099       0.998313           0.490901        0.053464   \n",
       "2            0.519321       0.814985           0.480679        0.506822   \n",
       "3            0.509099       0.833922           0.490901        0.493012   \n",
       "4            0.519321       0.998555           0.480679        0.054519   \n",
       "5            0.495206       0.890616           0.504794        0.504753   \n",
       "6            0.497313       0.892242           0.502687        0.503643   \n",
       "7            0.519321       0.883067           0.480679        0.514016   \n",
       "8            0.495206       0.998523           0.504794        0.053620   \n",
       "9            0.483157       0.842789           0.516843        0.500065   \n",
       "10           0.495206       0.839422           0.504794        0.498377   \n",
       "11           0.483157       0.999187           0.516843        0.051566   \n",
       "12           0.509099       0.890876           0.490901        0.508796   \n",
       "13           0.497313       0.835707           0.502687        0.498520   \n",
       "14           0.497313       0.998970           0.502687        0.050322   \n",
       "\n",
       "    train_brier_loss  \n",
       "0           0.162952  \n",
       "1           0.008207  \n",
       "2           0.170054  \n",
       "3           0.164236  \n",
       "4           0.008154  \n",
       "5           0.163415  \n",
       "6           0.162645  \n",
       "7           0.167794  \n",
       "8           0.008060  \n",
       "9           0.167397  \n",
       "10          0.166622  \n",
       "11          0.007906  \n",
       "12          0.164662  \n",
       "13          0.166283  \n",
       "14          0.007572  \n",
       "\n",
       "[15 rows x 60 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RESULTS_EVAL = []\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "for l in ['attention']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        model, pid = f[:f.index('.pkl')].split('#')\n",
    "        res = load(os.path.join(dir_l, f))\n",
    "        X, y = res.X_test, res.y_test\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_proba = res.estimator.predict_proba(X)\n",
    "        ev_test = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_proba=y_proba,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        X, y = res.X_train, res.y_train\n",
    "        y_pred = res.estimator.predict(X)\n",
    "        y_proba = res.estimator.predict_proba(X)\n",
    "        ev_train = evaluate(\n",
    "            y_true=y,\n",
    "            y_pred=y_pred,\n",
    "            y_proba=y_proba,\n",
    "            classes=[0, 1]\n",
    "        )\n",
    "\n",
    "        RESULTS_EVAL.append({\n",
    "            'label': l,\n",
    "            'alg': model,\n",
    "            'split': pid,\n",
    "            'n_feature': len(X.columns),\n",
    "            **{\n",
    "                f'test_{k}': v for k, v in ev_test.items()\n",
    "            },\n",
    "            **{\n",
    "                f'train_{k}': v for k, v in ev_train.items()\n",
    "                            }\n",
    "        })\n",
    "    \n",
    "RESULTS_EVAL = pd.DataFrame(RESULTS_EVAL)\n",
    "RESULTS_EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:32:23,329\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:26] Success: [rf_ns#fold4] Normalizing numeric features (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:26] Success: [rf_ns#fold2] Normalizing numeric features (0.06s).\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:26] Success: [rf_ns#fold5] Normalizing numeric features (0.06s).\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:26] Success: [rf_ns#fold3] Normalizing numeric features (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:26] Success: [rf_ns#fold1] Normalizing numeric features (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:26] Success: [rf_ns#fold5] 1-th Feature selection (0.75s).\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 7; # Num. = 51)\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:26] Success: [rf_ns#fold1] 1-th Feature selection (0.80s).\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 10; # Num. = 48)\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:26] Success: [rf_ns#fold4] 1-th Feature selection (0.87s).\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m - # Sel. Feat.: 63 (# Cat. = 8; # Num. = 55)\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:26] Success: [rf_ns#fold3] 1-th Feature selection (0.82s).\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m - # Sel. Feat.: 62 (# Cat. = 12; # Num. = 50)\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:26] In progress: [rf_ns#fold3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:27] Success: [rf_ns#fold2] 1-th Feature selection (0.94s).\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 6; # Num. = 52)\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:27] In progress: [rf_ns#fold2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:33] Success: [rf_ns#fold3] Training (7.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:34] Success: [rf_ns#fold1] Training (7.40s).\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:34] Success: [rf_ns#fold4] Training (7.95s).\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:34] Success: [rf_ns#fold5] Training (8.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:35] Success: [rf_ns#fold2] Training (7.96s).\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:35] In progress: [xgb_ns#fold3] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:35] In progress: [xgb_ns#fold1] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:35] Success: [xgb_ns#fold1] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:35] In progress: [xgb_ns#fold1] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:35] In progress: [xgb_ns#fold2] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:35] Success: [xgb_ns#fold2] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:35] In progress: [xgb_ns#fold2] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:35] Success: [xgb_ns#fold3] Normalizing numeric features (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:35] In progress: [xgb_ns#fold3] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:35] In progress: [xgb_ns#fold5] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:35] Success: [xgb_ns#fold5] Normalizing numeric features (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:35] In progress: [xgb_ns#fold5] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:35] In progress: [xgb_ns#fold4] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:35] Success: [xgb_ns#fold4] Normalizing numeric features (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:35] In progress: [xgb_ns#fold4] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:36] Success: [xgb_ns#fold1] 1-th Feature selection (0.75s).\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 10; # Num. = 48)\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:36] In progress: [xgb_ns#fold1] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:36] Success: [xgb_ns#fold3] 1-th Feature selection (0.75s).\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m - # Sel. Feat.: 62 (# Cat. = 12; # Num. = 50)\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:36] In progress: [xgb_ns#fold3] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:36] Success: [xgb_ns#fold2] 1-th Feature selection (0.88s).\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 6; # Num. = 52)\n",
      "\u001b[2m\u001b[36m(_train pid=87455)\u001b[0m [25-06-03 23:32:36] In progress: [xgb_ns#fold2] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:36] Success: [xgb_ns#fold5] 1-th Feature selection (0.70s).\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m - # Sel. Feat.: 58 (# Cat. = 7; # Num. = 51)\n",
      "\u001b[2m\u001b[36m(_train pid=87461)\u001b[0m [25-06-03 23:32:36] In progress: [xgb_ns#fold5] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:36] Success: [xgb_ns#fold4] 1-th Feature selection (0.74s).\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m - # Orig. Feat.: 84 (# Cat. = 16; # Num. = 68)\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m - # Sel. Feat.: 63 (# Cat. = 8; # Num. = 55)\n",
      "\u001b[2m\u001b[36m(_train pid=87456)\u001b[0m [25-06-03 23:32:36] In progress: [xgb_ns#fold4] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=87452)\u001b[0m [25-06-03 23:32:37] Success: [xgb_ns#fold1] Training (1.60s).\n",
      "\u001b[2m\u001b[36m(_train pid=87454)\u001b[0m [25-06-03 23:32:37] Success: [xgb_ns#fold3] Training (1.57s).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_LOGREG = LogisticRegression(\n",
    "    penalty='l2', solver='liblinear', random_state=RANDOM_STATE\n",
    ")\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE, max_depth=5, class_weight='balanced')\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    max_depth=3,\n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    early_stopping_rounds=10, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "    estimator=LinearSVC(\n",
    "        penalty='l1',\n",
    "        loss='squared_hinge',\n",
    "        dual=False,\n",
    "        tol=1e-3,\n",
    "        C=1e-2,\n",
    "        max_iter=5000,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    threshold=1e-5\n",
    ")\n",
    "\n",
    "CLS = ['attention']\n",
    "SETTINGS = [\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_RF),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='rf_ns'\n",
    "    ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_RF),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='rf_os'\n",
    "#     ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=False,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_ns'\n",
    "    ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_XGB),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='xgb_os'\n",
    "#     )\n",
    "]\n",
    "\n",
    "with on_ray(num_cpus=12):\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):\n",
    "        p = os.path.join(PATH_INTERMEDIATE, f'{l}.pkl')\n",
    "        par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', l)\n",
    "        os.makedirs(par_dir, exist_ok=True)\n",
    "        \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups,\n",
    "            path=par_dir,\n",
    "            categories=cats,\n",
    "            normalize=True,\n",
    "            split='stratifiedgroupkfold',\n",
    "            split_params={'n_splits': 5}, # logo시 삭제\n",
    "            random_state=RANDOM_STATE,\n",
    "            **s\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th>metric</th>\n",
       "      <th>n</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>value_count</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>SD</th>\n",
       "      <th>med</th>\n",
       "      <th>range</th>\n",
       "      <th>conf.</th>\n",
       "      <th>nan_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention</td>\n",
       "      <td>logreg</td>\n",
       "      <td>split</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>fold4:1, fold5:1, fold2:1, fold3:1, fold1:1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attention</td>\n",
       "      <td>logreg</td>\n",
       "      <td>n_feature</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>(84, 84)</td>\n",
       "      <td>(nan, nan)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attention</td>\n",
       "      <td>logreg</td>\n",
       "      <td>test_inst</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54999.000000</td>\n",
       "      <td>10999.800000</td>\n",
       "      <td>793.205333</td>\n",
       "      <td>11193.000000</td>\n",
       "      <td>(9996, 12012)</td>\n",
       "      <td>(10014.905495065195, 11984.694504934803)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attention</td>\n",
       "      <td>logreg</td>\n",
       "      <td>test_inst_0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27552.000000</td>\n",
       "      <td>5510.400000</td>\n",
       "      <td>928.119227</td>\n",
       "      <td>5859.000000</td>\n",
       "      <td>(4410, 6468)</td>\n",
       "      <td>(4357.987769477046, 6662.8122305229535)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>attention</td>\n",
       "      <td>logreg</td>\n",
       "      <td>test_inst_1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27447.000000</td>\n",
       "      <td>5489.400000</td>\n",
       "      <td>449.566791</td>\n",
       "      <td>5355.000000</td>\n",
       "      <td>(4893, 6027)</td>\n",
       "      <td>(4931.189100233543, 6047.610899766456)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>attention</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>train_prauc_ref_0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.504097</td>\n",
       "      <td>0.500819</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>0.497313</td>\n",
       "      <td>(0.48315688161693937, 0.5193213949104618)</td>\n",
       "      <td>(0.48362952511374857, 0.5180092537779024)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>attention</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>train_prauc_1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.993549</td>\n",
       "      <td>0.998710</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.998555</td>\n",
       "      <td>(0.9983127835822079, 0.9991874911918566)</td>\n",
       "      <td>(0.9982653383377209, 0.9991542005486689)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>attention</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>train_prauc_ref_1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.495903</td>\n",
       "      <td>0.499181</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>0.502687</td>\n",
       "      <td>(0.4806786050895382, 0.5168431183830606)</td>\n",
       "      <td>(0.48199074622209753, 0.5163704748862513)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>attention</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>train_log_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.263491</td>\n",
       "      <td>0.052698</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.053464</td>\n",
       "      <td>(0.050322293319982135, 0.05451939035842228)</td>\n",
       "      <td>(0.05057651159186952, 0.05482000268592851)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>attention</td>\n",
       "      <td>xgb_ns</td>\n",
       "      <td>train_brier_loss</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039899</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>(0.007572472225895065, 0.008207145859636358)</td>\n",
       "      <td>(0.007663328717335164, 0.008296332357789166)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label     alg             metric  n  cardinality  \\\n",
       "0    attention  logreg              split  5          5.0   \n",
       "1    attention  logreg          n_feature  5          NaN   \n",
       "2    attention  logreg          test_inst  5          NaN   \n",
       "3    attention  logreg        test_inst_0  5          NaN   \n",
       "4    attention  logreg        test_inst_1  5          NaN   \n",
       "..         ...     ...                ... ..          ...   \n",
       "169  attention  xgb_ns  train_prauc_ref_0  5          NaN   \n",
       "170  attention  xgb_ns      train_prauc_1  5          NaN   \n",
       "171  attention  xgb_ns  train_prauc_ref_1  5          NaN   \n",
       "172  attention  xgb_ns     train_log_loss  5          NaN   \n",
       "173  attention  xgb_ns   train_brier_loss  5          NaN   \n",
       "\n",
       "                                     value_count           sum          mean  \\\n",
       "0    fold4:1, fold5:1, fold2:1, fold3:1, fold1:1           NaN           NaN   \n",
       "1                                            NaN    420.000000     84.000000   \n",
       "2                                            NaN  54999.000000  10999.800000   \n",
       "3                                            NaN  27552.000000   5510.400000   \n",
       "4                                            NaN  27447.000000   5489.400000   \n",
       "..                                           ...           ...           ...   \n",
       "169                                          NaN      2.504097      0.500819   \n",
       "170                                          NaN      4.993549      0.998710   \n",
       "171                                          NaN      2.495903      0.499181   \n",
       "172                                          NaN      0.263491      0.052698   \n",
       "173                                          NaN      0.039899      0.007980   \n",
       "\n",
       "             SD           med                                         range  \\\n",
       "0           NaN           NaN                                           NaN   \n",
       "1      0.000000     84.000000                                      (84, 84)   \n",
       "2    793.205333  11193.000000                                 (9996, 12012)   \n",
       "3    928.119227   5859.000000                                  (4410, 6468)   \n",
       "4    449.566791   5355.000000                                  (4893, 6027)   \n",
       "..          ...           ...                                           ...   \n",
       "169    0.013844      0.497313     (0.48315688161693937, 0.5193213949104618)   \n",
       "170    0.000358      0.998555      (0.9983127835822079, 0.9991874911918566)   \n",
       "171    0.013844      0.502687      (0.4806786050895382, 0.5168431183830606)   \n",
       "172    0.001709      0.053464   (0.050322293319982135, 0.05451939035842228)   \n",
       "173    0.000255      0.008060  (0.007572472225895065, 0.008207145859636358)   \n",
       "\n",
       "                                            conf.  nan_count  \n",
       "0                                             NaN        NaN  \n",
       "1                                      (nan, nan)        0.0  \n",
       "2        (10014.905495065195, 11984.694504934803)        0.0  \n",
       "3         (4357.987769477046, 6662.8122305229535)        0.0  \n",
       "4          (4931.189100233543, 6047.610899766456)        0.0  \n",
       "..                                            ...        ...  \n",
       "169     (0.48362952511374857, 0.5180092537779024)        0.0  \n",
       "170      (0.9982653383377209, 0.9991542005486689)        0.0  \n",
       "171     (0.48199074622209753, 0.5163704748862513)        0.0  \n",
       "172    (0.05057651159186952, 0.05482000268592851)        0.0  \n",
       "173  (0.007663328717335164, 0.008296332357789166)        0.0  \n",
       "\n",
       "[174 rows x 13 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "SUMMARY_EVAL = []\n",
    "\n",
    "for row in RESULTS_EVAL.groupby(\n",
    "    ['label', 'alg']\n",
    ").agg(summary).reset_index().itertuples():\n",
    "    for k, v in row._asdict().items():\n",
    "        if type(v) is dict:\n",
    "            r = dict(\n",
    "                label=row.label,\n",
    "                alg=row.alg,\n",
    "                metric=k,\n",
    "                **v\n",
    "            )\n",
    "            SUMMARY_EVAL.append(r)\n",
    "\n",
    "SUMMARY_EVAL = pd.DataFrame(SUMMARY_EVAL)    \n",
    "SUMMARY_EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows metrics of our interest only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"13\" halign=\"left\">mean_sd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>n_feature</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_f1_0</th>\n",
       "      <th>test_f1_1</th>\n",
       "      <th>test_f1_macro</th>\n",
       "      <th>test_inst_0</th>\n",
       "      <th>test_inst_1</th>\n",
       "      <th>train_class_ratio</th>\n",
       "      <th>train_f1_0</th>\n",
       "      <th>train_f1_1</th>\n",
       "      <th>train_f1_macro</th>\n",
       "      <th>train_inst_0</th>\n",
       "      <th>train_inst_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>alg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">attention</th>\n",
       "      <th>logreg</th>\n",
       "      <td>84.0 (0.0)</td>\n",
       "      <td>0.726 (0.02)</td>\n",
       "      <td>0.724 (0.035)</td>\n",
       "      <td>0.723 (0.038)</td>\n",
       "      <td>0.723 (0.018)</td>\n",
       "      <td>5510.4 (928.119)</td>\n",
       "      <td>5489.4 (449.567)</td>\n",
       "      <td>1.005 (0.056)</td>\n",
       "      <td>0.739 (0.009)</td>\n",
       "      <td>0.738 (0.012)</td>\n",
       "      <td>0.738 (0.006)</td>\n",
       "      <td>22041.6 (928.119)</td>\n",
       "      <td>21957.6 (449.567)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_ns</th>\n",
       "      <td>59.8 (2.49)</td>\n",
       "      <td>0.731 (0.026)</td>\n",
       "      <td>0.685 (0.044)</td>\n",
       "      <td>0.763 (0.032)</td>\n",
       "      <td>0.724 (0.026)</td>\n",
       "      <td>5510.4 (928.119)</td>\n",
       "      <td>5489.4 (449.567)</td>\n",
       "      <td>1.005 (0.056)</td>\n",
       "      <td>0.751 (0.015)</td>\n",
       "      <td>0.81 (0.008)</td>\n",
       "      <td>0.78 (0.008)</td>\n",
       "      <td>22041.6 (928.119)</td>\n",
       "      <td>21957.6 (449.567)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_ns</th>\n",
       "      <td>59.8 (2.49)</td>\n",
       "      <td>0.738 (0.01)</td>\n",
       "      <td>0.73 (0.02)</td>\n",
       "      <td>0.741 (0.034)</td>\n",
       "      <td>0.736 (0.01)</td>\n",
       "      <td>5510.4 (928.119)</td>\n",
       "      <td>5489.4 (449.567)</td>\n",
       "      <td>1.005 (0.056)</td>\n",
       "      <td>0.995 (0.0)</td>\n",
       "      <td>0.995 (0.0)</td>\n",
       "      <td>0.995 (0.0)</td>\n",
       "      <td>22041.6 (928.119)</td>\n",
       "      <td>21957.6 (449.567)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean_sd                                               \\\n",
       "metric              n_feature       test_acc      test_f1_0      test_f1_1   \n",
       "label     alg                                                                \n",
       "attention logreg   84.0 (0.0)   0.726 (0.02)  0.724 (0.035)  0.723 (0.038)   \n",
       "          rf_ns   59.8 (2.49)  0.731 (0.026)  0.685 (0.044)  0.763 (0.032)   \n",
       "          xgb_ns  59.8 (2.49)   0.738 (0.01)    0.73 (0.02)  0.741 (0.034)   \n",
       "\n",
       "                                                                     \\\n",
       "metric            test_f1_macro       test_inst_0       test_inst_1   \n",
       "label     alg                                                         \n",
       "attention logreg  0.723 (0.018)  5510.4 (928.119)  5489.4 (449.567)   \n",
       "          rf_ns   0.724 (0.026)  5510.4 (928.119)  5489.4 (449.567)   \n",
       "          xgb_ns   0.736 (0.01)  5510.4 (928.119)  5489.4 (449.567)   \n",
       "\n",
       "                                                                  \\\n",
       "metric           train_class_ratio     train_f1_0     train_f1_1   \n",
       "label     alg                                                      \n",
       "attention logreg     1.005 (0.056)  0.739 (0.009)  0.738 (0.012)   \n",
       "          rf_ns      1.005 (0.056)  0.751 (0.015)   0.81 (0.008)   \n",
       "          xgb_ns     1.005 (0.056)    0.995 (0.0)    0.995 (0.0)   \n",
       "\n",
       "                                                                       \n",
       "metric           train_f1_macro       train_inst_0       train_inst_1  \n",
       "label     alg                                                          \n",
       "attention logreg  0.738 (0.006)  22041.6 (928.119)  21957.6 (449.567)  \n",
       "          rf_ns    0.78 (0.008)  22041.6 (928.119)  21957.6 (449.567)  \n",
       "          xgb_ns    0.995 (0.0)  22041.6 (928.119)  21957.6 (449.567)  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUB_SUMMARY_EVAL = SUMMARY_EVAL.loc[\n",
    "    lambda x: x['metric'].isin(\n",
    "        ['n_feature', 'train_class_ratio', 'train_inst_0', 'train_inst_1', 'test_inst_0', 'test_inst_1', 'test_acc', 'test_f1_0' ,'test_f1_1', 'test_f1_macro', 'train_f1_0' ,'train_f1_1', 'train_f1_macro',]\n",
    "    )\n",
    "].round(3).assign(\n",
    "    mean_sd=lambda x: x['mean'].astype(str).str.cat(' (' + x['SD'].astype(str) + ')', sep='')\n",
    ").pivot(\n",
    "    index=['label', 'alg'], columns=['metric'], values=['mean_sd']\n",
    ")\n",
    "SUB_SUMMARY_EVAL.to_csv('./fig/SUB_SUMMARY_EVAL.csv')\n",
    "SUB_SUMMARY_EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# 디렉토리 생성\n",
    "os.makedirs('./fig/confusion_matrices', exist_ok=True)\n",
    "\n",
    "# metric 분리\n",
    "cm_metrics = ['true_0_pred_0', 'true_0_pred_1', 'true_1_pred_0', 'true_1_pred_1']\n",
    "f1_metrics = ['f1_macro', 'f1_micro']\n",
    "cm_metrics = [f'test_{m}' for m in cm_metrics]\n",
    "f1_metrics = [f'test_{m}' for m in f1_metrics]\n",
    "\n",
    "# pivot\n",
    "mean_cm_df = SUMMARY_EVAL[SUMMARY_EVAL['metric'].isin(cm_metrics)].pivot(index=['label', 'alg'], columns='metric', values='mean')\n",
    "std_cm_df = SUMMARY_EVAL[SUMMARY_EVAL['metric'].isin(cm_metrics)].pivot(index=['label', 'alg'], columns='metric', values='SD')\n",
    "f1_df = SUMMARY_EVAL[SUMMARY_EVAL['metric'].isin(f1_metrics)].pivot(index=['label', 'alg'], columns='metric', values='mean')\n",
    "\n",
    "# 시각화 및 저장\n",
    "for idx in mean_cm_df.index:\n",
    "    try:\n",
    "        mean_cm = np.array([\n",
    "            [mean_cm_df.loc[idx, 'test_true_0_pred_0'], mean_cm_df.loc[idx, 'test_true_0_pred_1']],\n",
    "            [mean_cm_df.loc[idx, 'test_true_1_pred_0'], mean_cm_df.loc[idx, 'test_true_1_pred_1']]\n",
    "        ])\n",
    "        std_cm = np.array([\n",
    "            [std_cm_df.loc[idx, 'test_true_0_pred_0'], std_cm_df.loc[idx, 'test_true_0_pred_1']],\n",
    "            [std_cm_df.loc[idx, 'test_true_1_pred_0'], std_cm_df.loc[idx, 'test_true_1_pred_1']]\n",
    "        ])\n",
    "        annotations = np.empty_like(mean_cm, dtype=object)\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                annotations[i, j] = f\"{mean_cm[i, j]:.1f}±{std_cm[i, j]:.1f}\"\n",
    "\n",
    "        # f1 score 값\n",
    "        macro_f1 = f1_df.loc[idx, 'test_f1_macro']\n",
    "        micro_f1 = f1_df.loc[idx, 'test_f1_micro'] if 'test_f1_micro' in f1_df.columns else None\n",
    "\n",
    "        # 플롯\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(mean_cm, annot=annotations, fmt='', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "        plt.title(f'Confusion Matrix: {idx[0]} | {idx[1]} (mean ± std)')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.ylabel('True label')\n",
    "\n",
    "        # 텍스트로 F1 표시\n",
    "        f1_text = f\"Macro F1: {macro_f1:.3f}\"\n",
    "        if micro_f1 is not None:\n",
    "            f1_text += f\" | Micro F1: {micro_f1:.3f}\"\n",
    "        plt.figtext(0.5, -0.05, f1_text, ha='center', fontsize=10)\n",
    "\n",
    "        # 저장\n",
    "        filename = f\"confmat_{idx[0]}_{idx[1]}.png\".replace(\" \", \"_\")\n",
    "        filepath = os.path.join('./fig/confusion_matrices', filename)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {idx} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    split  test_inst_0  test_inst_1  test_f1_1\n",
      "0   fold2         6468         4893   0.729050\n",
      "5   fold3         5859         5334   0.779119\n",
      "6   fold1         6174         5838   0.764498\n",
      "7   fold4         4410         6027   0.805702\n",
      "12  fold5         4641         5355   0.734243\n"
     ]
    }
   ],
   "source": [
    "worst_splits = RESULTS_EVAL[\n",
    "    (RESULTS_EVAL['label'] == 'attention') &\n",
    "    (RESULTS_EVAL['alg'] == 'rf_ns') &\n",
    "    (RESULTS_EVAL['test_f1_1'] < 1.1)\n",
    "]\n",
    "print(worst_splits[['split', 'test_inst_0', 'test_inst_1', 'test_f1_1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "def feature_importance(\n",
    "    estimator\n",
    "):\n",
    "    if not hasattr(estimator, 'feature_names_in_') or not hasattr(estimator, 'feature_importances_'):\n",
    "        return None\n",
    "    \n",
    "    names = estimator.feature_names_in_\n",
    "    importances = estimator.feature_importances_\n",
    "    \n",
    "    return names, importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "IMPORTANCE_EVAL = defaultdict(list)\n",
    "DIR_EVAL = os.path.join(PATH_INTERMEDIATE, 'eval')\n",
    "\n",
    "for l in ['attention']:\n",
    "    dir_l = os.path.join(DIR_EVAL, l)\n",
    "    if not os.path.exists(dir_l):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(dir_l):\n",
    "        res = load(os.path.join(dir_l, f))\n",
    "\n",
    "        f_norm = f[:f.index('.pkl')]\n",
    "        alg = f_norm[:f.rindex('#')]\n",
    "        \n",
    "        feat_imp = feature_importance(res.estimator)\n",
    "        if not feat_imp:\n",
    "            continue\n",
    "            \n",
    "        names, importance = feat_imp\n",
    "        new_names = []\n",
    "        for n in names:\n",
    "            for c in res.categories:\n",
    "                n = n.replace(f'{c}_', f'{c}=')\n",
    "            new_names.append(n)\n",
    "        \n",
    "        d = pd.DataFrame(\n",
    "            importance.reshape(1, -1),\n",
    "            columns=new_names\n",
    "        )\n",
    "        IMPORTANCE_EVAL[(l, alg)].append(d)\n",
    "        \n",
    "\n",
    "IMPORTANCE_SUMMARY = []\n",
    "\n",
    "for (l, alg), v in IMPORTANCE_EVAL.items():\n",
    "    new_v = pd.concat(\n",
    "        v, axis=0\n",
    "    ).fillna(0.0).mean().reset_index().set_axis(\n",
    "        ['feature', 'importance'], axis=1\n",
    "    ).assign(\n",
    "        label=l,\n",
    "        alg=alg\n",
    "    )\n",
    "    IMPORTANCE_SUMMARY.append(new_v)\n",
    "    \n",
    "IMPORTANCE_SUMMARY = pd.concat(IMPORTANCE_SUMMARY, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%R` not found.\n"
     ]
    }
   ],
   "source": [
    "%%R -i IMPORTANCE_SUMMARY -w 26 -h 16 -u cm\n",
    "\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(stringr)\n",
    "library(patchwork)\n",
    "\n",
    "data <- IMPORTANCE_SUMMARY %>% filter(label == 'attention')\n",
    "\n",
    "p_label <- ggplot() + geom_text(\n",
    "    aes(x = .5, y = .5),\n",
    "    label = 'Attention',\n",
    "    family = 'ssp',\n",
    "    fontface = 'bold',\n",
    "    size = 4\n",
    ") + theme_void()\n",
    "\n",
    "p_rf <- ggplot(\n",
    "    data %>% filter(alg == 'rf_os') %>% top_n(n = 10, wt = importance),\n",
    "    aes(x = reorder(feature, -importance), y = importance)\n",
    ") + geom_col() +\n",
    "    THEME_DEFAULT + theme(\n",
    "        axis.text.x = element_text(angle = 90, size = 10, hjust = 1, vjust = .5),\n",
    "        axis.title.x = element_blank(),\n",
    "        axis.title.y = element_blank()\n",
    "    ) + labs(subtitle = 'Random Forest')\n",
    "\n",
    "p_xgb <- ggplot(\n",
    "    data %>% filter(alg == 'xgb_os') %>% top_n(n = 10, wt = importance),\n",
    "    aes(x = reorder(feature, -importance), y = importance)\n",
    ") + geom_col() +\n",
    "    THEME_DEFAULT + theme(\n",
    "        axis.text.x = element_text(angle = 90, size = 10, hjust = 1, vjust = .5),\n",
    "        axis.title.x = element_blank(),\n",
    "        axis.title.y = element_blank()\n",
    "    ) + labs(subtitle = 'XGBoost')\n",
    "\n",
    "p <- p_label / (p_rf | p_xgb) + plot_layout(heights = c(1.1, 10))\n",
    "\n",
    "ggsave('./fig/imp_attention.pdf', plot = p, width = 26, height = 16, unit = 'cm', device = cairo_pdf)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ray-env)",
   "language": "python",
   "name": "ray-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
